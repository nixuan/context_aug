{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from llama_index import GPTSimpleVectorIndex, GPTListIndex, GPTTreeIndex, SimpleDirectoryReader, LLMPredictor\n",
    "from llama_index import download_loader\n",
    "from pathlib import Path\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"gpt-3.5-turbo\", max_tokens=1300))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(text='封面页 \\n\\n（此页面将由下图全覆盖，此为编辑稿中的示意，将在终稿 PDF 版中做更新） \\n\\n\\x0c\\n作者简介 \\n\\n作者：刘伟光 \\n\\n阿里巴巴集团副总裁、阿里云智能新金融&互联网事业部总经理，毕业于清华大学\\n\\n电子工程系。 \\n\\n加入阿里云之前，在蚂蚁金服负责金融科技的商业推广和生态建设工作以及蚂蚁区\\n\\n块链的商业拓展工作；在企业软件市场深耕多年，曾经创建 Pivotal 软件大中华区分\\n\\n公司，开创了企业级大数据以及企业级云计算 PaaS 平台的市场先河。 \\n\\n在创建 Pivotal 中国软件公司之前，刘伟光曾经担任 EMC 大中国区数据计算事业部\\n\\n总经理，并在甲骨文中国公司工作多年，曾经创建了 Exadata 大中国区的产品事业\\n\\n部并担任事业部总监。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n专家力荐 \\n\\n过去几年，阿里云、瓴羊等平台和企业，致力于将阿里十数年沉淀的数字化经验，\\n\\n帮助企业尤其是金融机构，通过数据智能落地，探索业务的不断增长。今天，这些\\n\\n经验和思考集结成册，相信无论对金融行业，还是每一个力求数字化的企业，都是\\n\\n极好的经验交流的机会。信息（数据）只有流动起来，才能发挥出最大的价值。数\\n\\n据的未来，就是我们的未来。 \\n\\n——阿里巴巴集团副总裁、瓴羊 CEO  朋新宇 \\n\\n在过去的二十年，我们看到一个越来越明显的趋势，数据要素正在转化成为金融企\\n\\n业新的“资产”，在风险防范、客户服务、产品推广等领域发挥着不可或缺的作用，\\n\\n助推金融企业的业务发展。但是数据并不会自发地转化为资产，业界的成功经验表\\n\\n明，这背后需要建设一套完整的数据体系，并且需要有体系化的建设方法，否则企\\n\\n业会深受数据质量差、底数不清、数据更新不及时、数据口径不一致等问题的困扰。\\n\\n该文基于阿里云多年服务金融行业的成功经验，针对金融企业在数据领域可能遭遇\\n\\n的挑战，从多个维度阐述了数据体系的建设，并且总结了一套完整的数据体系建设\\n\\n方法，对于金融企业而言有很强的借鉴意义。 \\n\\n——阿里云智能副总裁、行业解决方案研发部总经理  曾震宇 \\n\\n一个看似稳定的行业会在什么时候迎来重大的业态改变？可能是新的生产要素出现\\n\\n以及配合新要素的生产工具也出现新发展的时候，这个转变恰好刚刚开始。数据作\\n\\n为新要素获得社会级的支持是近两年的事情，从数据要素统一大市场到数据资产记\\n\\n账规则再到“数据二十条”的发布，我们看到的是国家为新要素发展做出的基础性\\n\\n改变，但企业是否已经真的意识到了新要素的重要性？是否为此准备好了新工具？\\n\\n是否为新工具调适了新组织？可能大部分企业尚未做好这样的准备，也在思考要不\\n\\n要做、要怎么做。阿里云既经历了阿里自身的新要素建设，也观察了多个行业的新\\n\\n要素探索，本次阿里云将经验与思考合盘托出，是一次对数据要素发展的“盘评治\\n\\n享”，相信企业会在此找到同行者。 \\n\\n——北京天润聚粮咨询执行董事总经理、中国计算机学会软件工程专委会委员  付晓岩 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n目录 \\n\\n引言 ...................................................................................... 5 \\n\\n一、金融行业数据领域面临的困扰 .............................................. 7 \\n\\n1.  数据平台逐渐“掉队” ................................................................................... 7 \\n\\n2.  数据治理面临“熵增”困境 ........................................................................... 8 \\n\\n3.  数据资产面临“兵多、精兵少” .................................................................... 9 \\n\\n4.  数据服务“效率低” ....................................................................................... 9 \\n\\n二、金融数据发展瓶颈的破题之术 ............................................ 11 \\n\\n1.  转换顶层设计思维 ......................................................................................... 11 \\n\\n2.  锚定业务价值目标 ......................................................................................... 13 \\n\\n3.  破除数据供需壁垒 ......................................................................................... 14 \\n\\n4.  严控数据高质提效 ......................................................................................... 15 \\n\\n5.  融合算力数智驱动 ......................................................................................... 16 \\n\\n6.  强化数字人才能力 ......................................................................................... 17 \\n\\n三、金融数据能力建设的核心观点 ............................................ 19 \\n\\n1.  以全局视角的驱动力 ..................................................................................... 19 \\n\\n2.  以分层视角的核心力 ..................................................................................... 23 \\n\\n3.  以业务视角的价值链 ..................................................................................... 48 \\n\\n四、金融数据能力建设的成功要素 ............................................ 57 \\n\\n1.  关键能力和价值方向 ..................................................................................... 57 \\n\\n2.  关键路径和成功要素 ..................................................................................... 58 \\n\\n尾声 .................................................................................... 64 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                                                                             引言 \\n\\n5 \\n\\n引言 \\n\\n“大风起于青萍之末，浪成于微澜之间。”用这句话来形容中国金融行业数据领域\\n\\n发展的历程颇为恰当。 \\n\\n从 2003 年银行的数据仓库开始建设到今天整整 20 年过去了，相比其他行业，金融\\n\\n行业是真正完整地经历了数据仓库建设和大数据建设的两个十年周期；同时经历了\\n\\n从 MPP 技术到 Hadoop 大数据开源技术，再到存储计算分离的云原生数据处理技\\n\\n术；再到 AI 数据智能化的时代；从持续了 20 年的数据治理到今天全域数据资产管\\n\\n理的数据中台化架构，关于金融行业的数据计算分析和智能化运营的探索从来没有\\n\\n停止过。但是如何在原有的数据基础和平台技术积累上完成全局视角的数据能力提\\n\\n升和技术体系的迭代，这是今天金融行业共同关注的话题，也是金融机构进入数字\\n\\n化智能时代的重要标志。 \\n\\n阿里云历时五年助力阿里巴巴集团完成了全域数据中台的建设，同时也服务了众多\\n\\n不同类型的金融行业客户在数据平台的建设和数据治理；数据智能化方面，在接触\\n\\n了近千家金融机构了解实际业务需求和现状后，激发了我们创造本文的热情与初衷，\\n\\n期望用更全面、更立体的视角去定义数据的技术和业务价值；站在全局视角去剖析\\n\\n数据生命周期的管理；站在云原生技术的视角去审视数据计算能力的未来布局。 \\n\\n同时全文解析了从底层数据计算到数据资产化的完整建设方法和路径，分析了当前\\n\\n数据平台如何从分而治之的建设模式和技术体系逐步演进到全局的数据智能化中台。\\n\\n关于数据领域研究，不仅仅是单纯的技术命题，我们也讨论了金融机构内部数据运\\n\\n营模式以及数据人才建设体系这些当前重要话题。 \\n\\n期待本文能够为金融机构不同部门在治理/应用/运营/计算/决策等诸多数据方面的\\n\\n工作，带来有价值的新发现，带来更多关于数据思想的碰撞。 \\n\\n本文共分为四个章节，全面围绕当前金融机构在经历近 20 年的数据建设中，依然\\n\\n面临的效率、质量、服务、人才等方面问题，核心从业务对数据的痛点入手，通过\\n\\n能力体系建设方法，助力金融机构掌握数据能力体系的建设目标、方法和成功要素，\\n\\n从而推动金融机构逐步实现数据驱动业务发展的目标： \\n\\n第一章从金融行业数据领域面临的问题出发，重点描述当前金融机构在经历近 20 年\\n\\n的数据领域建设后，仍然面临数据底数摸不清、数据治理差、数据服务低效、数据\\n\\n资源不足和响应慢的问题，而这些都是我们面对不同金融机构、不同层级的客户提\\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                                                                             引言 \\n\\n6 \\n\\n出的痛点和困惑。我们综合了相关问题，通过部分示例列举的方式给出了相关的判\\n\\n断。 \\n\\n第二章结合第一章分析的数据体系、数据质量、数据资产、数据服务等领域面临的\\n\\n困惑，提出采用全局思维，以业务价值为终极目标，分析数据领域问题，再进行分\\n\\n层分步推进的总体策略。同时，为了便于读者理解，引出数据建设的 6 大断言，涵\\n\\n盖了数据能力体系顶层设计、业务价值、数据服务、数据治理、数据平台、数字人\\n\\n才相关领域的建设方向，希望为读者带来启发和帮助。 \\n\\n第三章从建设方法入手，给出金融行业未来数据能力建设的核心观点，首先从全局\\n\\n视角规划企业数据能力体系的“1”张数据战略蓝图、“3+1”数据体系以及  “6”\\n\\n大核心能力，并对其中的数据体系提出了算力驱动、数据驱动和价值驱动的 3 大驱\\n\\n动力；其次用分层思维，深化“3+1”数据体系设计，提出了数字基建 5 大法则、\\n\\n数字资产 8 项能力、数字应用 3 个要素、运营保障 1 套机制的核心能力；最后站在\\n\\n金融机构的“前中后台”不同部门的视角，以获客营销、产品定价、风险审计、数\\n\\n据治理、技术架构等 5 个领域为例，分析数据体系为企业数字化经营带来的价值。 \\n\\n第四章结合第三章的数据体系建设方法，本章以目标为导向，开篇提出金融行业数\\n\\n据体系建设的 5 个价值方向，目的是为了让读者了解未来建设数据体系的成效目标。\\n\\n然后围绕数据体系化建设、数据模型合理选择、数仓建设模式、数据服务协同、数\\n\\n据资产运营、数据确信机制等方面给出相应的策略分析，帮助金融机构结合自身现\\n\\n状和现有成果进行选择，满足自身发展需求的建设路径和方法，或者是整体重构，\\n\\n也或者是升级优化。此外也建议金融机构在实施路径上，既要有战略层面的全局设\\n\\n计，也要能小步快跑，满足现阶段业务发展需求。合理的选择实施策略和制定短中\\n\\n长期建设目标，将成功助力金融机构数字化转型。 \\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         一、金融行业数据领域面临的困扰  \\n\\n7 \\n\\n一、金融行业数据领域面临的困扰 \\n\\n真正的竞争力，是把所有人都可能拥有的东西变成财富，让沙子变成硅，这才是核\\n\\n心。 \\n\\n—王坚《在线》 \\n\\n数据是战略资源。如何高效获取更多有用有价值的数据？如何让数据赋能业务创新？\\n\\n如何保持指标口径一致，让监管数据更加准确？如何让更多的业务人员懂数据，又\\n\\n如何让更多的普通人会使用数据？这些问题都是当前各金融机构在数据领域建设中\\n\\n普遍面临的困扰。 \\n\\n1.  数据平台逐渐“掉队” \\n\\n“哪里的数字化程度高，用户就往哪里迁徙。” \\n\\n数字化程度一个重要标志就是，“数据与业务的关系”是否能从“跟随”（事后分\\n\\n析）、走到“伴随”（实时分析）、再走到“引领”（智能服务）。数据平台能力\\n\\n越强大，将会越快步入数字化程度更高的阶段。 \\n\\n金融数据是大数据商业应用最早的数据源。早在 1996 年摩根大通银行就聘请数学\\n\\n家丹尼尔利用递归决策树统计方法，对抵押贷款用户进行统计分析，帮助银行找到\\n\\n可能提前还款或者未来不会还款的客户。国内金融机构以国有大行为代表，从建设\\n\\n数据仓库至今已有近 20 年的时间，大数据技术从商业产品到开源产品“百花齐放”，\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         一、金融行业数据领域面临的困扰  \\n\\n8 \\n\\n数据大集中推动了企业级数据平台的形成与发展，在发展的过程中，金融机构形成\\n\\n了一套相对完整的数据技术体系和管理体系，支撑了以监管报送、统计分析、业务\\n\\n决策为代表的应用场景，取得了一定的成果。但同时，业务发展对数据井喷式的需\\n\\n求，金融服务粒度正在从“大”到“小”到“微”，数据平台对业务起到“实时精\\n\\n准制导”作用，原有的数据架构逐渐显得“力不从心”。 \\n\\n数据需求和海量数据的激增使数据平台资源规模的爆发式增加，大型金融机构节点\\n\\n规模达到数百甚至上千台，大规模集群的稳定性与数据容灾等问题凸显；面对异构\\n\\n数据的集成，差异化的数据格式分析，如何满足多样化应用的数据服务需求存在挑\\n\\n战。如，海量数据查询难以高质高效；多源数据统一存储管理，需要非常简单的融\\n\\n合分析能力；源系统数据变更频繁，需要自动识别和管理；全量建仓或者直连分析\\n\\n对源库压力较大，卸载压力规避故障，建仓延时长，需要低延时入湖；海量数据在\\n\\n事务库或者传统数仓中存在成本高，面临降本压力等等。 \\n\\n过去 5 年金融机构的数据总量增长了 3 倍多，实时数据、触点数据、过程数据、流\\n\\n数据、音视频（非结构化)数据等数据价值不断被挖掘，未来业务对数据规模、数据\\n\\n类型、数据应用需求会更进一步放大，现有的数据架构开始出现“掉队”情况。 \\n\\n2.  数据治理面临“熵增”困境 \\n\\n数据天然面临着“熵增”（模型、指标、数据量）问题，需要一个“都江堰工程”，\\n\\n持续治理、持续做减法。 \\n\\n金融行业是最早具有数据管理意识、最早开始建设数据治理体系的行业。虽然经过\\n\\n十几年数据标准建设，但金融机构中的数据仍然很难实现统一和规范，管理决策中\\n\\n数据普遍存在“同义不同名、同名不同义”的局面。例如，银行在监管报送和内部\\n\\n管理两个场景都使用“涉农贷款”这一指标，但是因为口径不同计算的结果肯定不\\n\\n同。监管报送中“涉农贷款”的口径只统计“涉及用于农业、林、牧、副、渔业所\\n\\n属活动或支农活动的贷款”，而内部管理的指标还要包含新农村建设、集体经济发\\n\\n展、民俗旅游等维度，因此出现“同名不同口径”的现象。这类同名指标虽有标准\\n\\n定义，但计算加工过程因没有管控，导致不同部门应用指标时会产生差错。 \\n\\n造成数据标准“只有定义、没有落地”的一个非常核心的问题在于，数据标准的“定\\n\\n义”和“实现”实际是两层皮，标准一旦定义完成，就变成“墙上的画”，没有切\\n\\n实镶嵌和固化到日常的数据生产、加工处理和应用的环节当中去，没能建立全链路\\n\\n数据治理能力，有效地从数据生产源头加以强制约束，确保数据质量。 \\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         一、金融行业数据领域面临的困扰  \\n\\n9 \\n\\n据监管公布数据：2021 年度，人民银行及银保监会向各类金融机构共开出“数据相\\n\\n关”的罚单超过 1000 张，其中与数据质量相关罚单超过 800 张，占罚单总量的近\\n\\n80%，位列处罚事由榜首。所有超千万大额罚单均与数据质量相关，数据质量直接\\n\\n影响了金融机构的生存质量，如何提高数据质量，已成为金融机构亟待解决的问题。 \\n\\n3.  数据资产面临“兵多、精兵少” \\n\\n“数据指标≠数据资产”。企业往往面临着“指标多、资产少”的困境，报表很多，\\n\\n但能够服务业务，且可复制、可重用的数据资产（精兵）却很少。 \\n\\n经过多年的信息化系统建设，一个中等规模的金融企业往往拥有数百个应用系统、\\n\\n多套数据平台、业务数据表数十万张、业务报表少则几百张、多则数千张。面对这\\n\\n样的“海量信息”，业务和技术人员都会存在一种“坐在金山上讨饭吃”而望数兴\\n\\n叹的感觉。 \\n\\n数据部门的数据分析师“找数难”：例如数据分析师建立分析模型时会困扰“没人\\n\\n能告诉我全貌数据都包含什么内容，我怎么能知道分析模型需要的数据咱们行是不\\n\\n是都有。” \\n\\n业务部门的数据用户“查数难”：“这个报表数据的内容看起来不对，我怀疑加工\\n\\n过程中有计算逻辑错误，可是科技也不清楚数据的加工链路，不知道错误出现在哪\\n\\n个环节。” \\n\\n科技部门的技术人员“用数难”：例如数据研发人员经常会遇到的困境“在五六个\\n\\n系统当中都有类似的客户信息数据，我用哪份数据去加工报表更合适？” \\n\\n事实上，这些问题的产生根源都在于我们没有对企业的“数据家底”进行全面的梳\\n\\n理和盘点，不同“数据用户”缺乏有效的工具和手段对企业的数据资产进行全面的\\n\\n了解。 \\n\\n4.  数据服务“效率低” \\n\\n拿破仑说“真理只在大炮射程之内”。同样，“数据价值也只在数据射程（数据服\\n\\n务）之内”。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         一、金融行业数据领域面临的困扰  \\n\\n10 \\n\\n“数据服务响应慢”是大部分金融机构用数部门面临的数据供给能力不足、效率低\\n\\n下的问题。在传统的数据服务模式中，业务部门与数据部门陷入到了“提需求——\\n\\n提数——未达预期——再提需求——再提数……”无限循环的“莫比乌斯怪圈”中。 \\n\\n这种工作模式中存在两个方面的关键问题：一方面是业务人员“懂业务不懂数据”，\\n\\n数据人员“懂数据不懂业务”，互相理解不够，传递层次越多、信息衰减越大。另\\n\\n一方面是数据需求往往靠技术团队逐个响应和满足（技术团队成为瓶颈），缺少统\\n\\n一的自助数据服务、共性数据需求提炼、差异化需求优先级管理等。目前，很多金\\n\\n融机构已经意识到这一点，开始提出“数据产品”、“数据门户”、“数据工厂”\\n\\n的理念来建设“自助化、可配置、可共享”的数据服务。 \\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         二、金融数据发展瓶颈的破题之术  \\n\\n11 \\n\\n二、金融数据发展瓶颈的破题之术 \\n\\n金融机构当前所面临的问题和挑战，不再是建设大数据的初期从无到有的过程，经\\n\\n过多年的建设，不论从技术、数据、应用各个维度或多或少已经沉淀了一定的数据\\n\\n基础，现阶段已经到了优化数据体系，推动数业融合，释放数据价值的阶段。 \\n\\n下面我们分别从顶层设计、业务价值、数据服务、数据治理、数据平台、数字人才\\n\\n6 个维度提出数据建设思路和断言，希望能够给大家带来一些启发和帮助。 \\n\\n1.  转换顶层设计思维 \\n\\n“未来商业的DNA有两个螺旋：一个是网络协作，另一个是数据智能。” \\n\\n——《智能商业》 \\n\\n断言 1：数据体系建设不再只是“科技的事”，“数据战略就是企业战略”。需要\\n\\n一把手站在“上帝视角”，有决心打破一定的传统业务惯性，推动业务、技术、组\\n\\n织来形成的企业级能力。 \\n\\n目前数据架构正在从“大数据”走向“快数据”，未来的金融机构都是“数据驱动\\n\\n型”企业，企业级数据架构需要数据覆盖更全，数据质量更高，数据流通更快，数\\n\\n据应用更准，加快数据与业务的深度融合。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         二、金融数据发展瓶颈的破题之术  \\n\\n12 \\n\\n1)  从业务视角 \\n\\n数据应用一定是业务场景与数据技术紧密融合的产物，如何让数据技术快速赋能业\\n\\n务，又如何缩短从业务提出数据需求到变现落地的时间窗口？是必须经过一个场景\\n\\n一个场景地打磨吗？显然不是。科学合理的方案是通过一个数据归集平台和一套高\\n\\n效的数据工具，让每个业务人员都能自主操作、自行分析，推动业务人员既是场景\\n\\n问题的提出者、也是场景方案的创作者。 \\n\\n2)  从技术视角 \\n\\n金融机构普遍在数据领域已经有了沉淀积累，但需要结合自身现状，形成一套覆盖\\n\\n数据采、建、管、用各个领域的“数据蓝图”。对数据采集而言，核心能力要做到\\n\\n全域数据实时采集，而“建”数据的核心能力是敏捷构建数据基础、数据资产和数\\n\\n据连接的能力，“管”数据的核心是实现全链路数据治理能力，“用”数据则是强\\n\\n调业务场景驱动的高效数据服务能力。此外，随着金融机构数据规模的激增，除了\\n\\n数据融通之外，平台的计算效率、存储成本、数据容灾、资源可扩展、可运维等方\\n\\n面都需要进行打造。 \\n\\n3)  从组织视角 \\n\\n打破部门墙，让数据团队走向业务团队，并建立跨部门协同机制，这是数据创造价\\n\\n值的一大利器。为此，很多金融机构提出了打破部门墙，培养综合人才的做法。有\\n\\n条件的金融机构纷纷成立了数据管理部门，统领全公司数据能力建设；也有些金融\\n\\n机构成立固定数据领域业务需求团队，对接业务部门，建立业务、科技的数据桥梁，\\n\\n并由此不断沉淀共性的数据需求。 \\n\\n在这样的组织形式下，数据部门与业务部门融为一体，建立专职人员组成协同团队，\\n\\n相互了解对方的工作内容和专业术语，数据应用的顺畅度大幅提升，业务部门的抱\\n\\n怨少了，数据部门的价值也就得到了体现。 \\n\\n由此可见，数据领域的建设不是单纯的技术建设，而是需要将技术、业务、数据以\\n\\n及组织与人才有机地结合起来，形成一个能力闭环，持续地沉淀数据能力并不断释\\n\\n放数据价值。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         二、金融数据发展瓶颈的破题之术  \\n\\n13 \\n\\n2.  锚定业务价值目标 \\n\\n“数据主义认为，万物皆算法，任何现象或实体的价值就在于其背后的数据与算法，\\n\\n企业运作就是一套数据+算法”。 \\n\\n——《未来简史》 \\n\\n断言 2：未来数字企业的核心竞争力在于，能否将业务的最佳实践“固化”为数据\\n\\n产品（数据和算法），形成一套业务实践与数据体系之间“闭环反馈”的“数字化\\n\\n神经系统”。 \\n\\n数据体系建设的成功与否，最终是以业务部门创造的业务价值来衡量和检验效果。\\n\\n数据体系一定要围绕着业务应用，以解决业务部门在不同场景中的痛点为根本目标。\\n\\n因此，引导业务部门积极主动参与建设，从各个业务场景出发设计数据应用方案是\\n\\n数据体系创造业务价值的关键因素。 \\n\\n以往金融机构的经营运转，特别是影响重大的业务运作极其依赖于人的知识与经验。\\n\\n有大量碎片化、难以结构化存储的知识是以经验的形式留存在一线员工的头脑中，\\n\\n这也造成了金融业务的开展很依赖于人，一些工作环节虽然无数次重复但却无法快\\n\\n速复制，一些关键决策点需要人的参与，无法保证前后多次决策的一致性。 \\n\\n例如：银行在进行贷前风险调查中，通常依赖于有经验的信贷经理实地调查。这些\\n\\n信贷经理常年与客户打交道，总结出了一套颇有成效的经验打法，但经验丰富的信\\n\\n贷经理毕竟是少数，大量年轻的信贷员要在老员工的传帮带下经过多年的锤炼才能\\n\\n足够胜任。像这样沉淀在信贷经理头脑中的业务经验，如果可以拆解到多种业务场\\n\\n景中，每个场景又包含了可量化的指标、可规则化的逻辑判断，以及可归纳总结的\\n\\n决策环节，最终再通过数字产品化的方法，为信贷经理提供直观的数据呈现和关键\\n\\n的决策点，信贷调查效率就能大大提升。 \\n\\n同样信贷业务，银行在贷后风险识别中，过去是由信贷经理上门查看企业生产运转\\n\\n情况，分析企业经营流水。有经验的信贷经理能从企业的工厂物料堆放，经常往来\\n\\n的供应商清单中看出风险痕迹。而现在，有些技术领先的银行已经在尝试通过 AI 视\\n\\n觉技术对企业生产现场照片和视频进行识别分析，采用 NLP 技术对企业在银行的流\\n\\n水记录进行分析，从而将信贷调查进行数字化、结构化，通过新技术尝试风险识别\\n\\n的智能化，从而辅助信贷人员提升风险判断能力。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         二、金融数据发展瓶颈的破题之术  \\n\\n14 \\n\\n数据的最大价值是通过对历史业务的挖掘分析，将以人为驱动的业务经验总结出来，\\n\\n形成以数据驱动的业务动作，用量化结果辅助业务决策。把人员从重复劳动中解放，\\n\\n让有经验的人员集中精力于业务决策。 \\n\\n3.  破除数据供需壁垒 \\n\\n“宇宙中最反直觉的真理是，你给别人的越多，你得到的也越多。” \\n\\n断言 3：数据与业务是相互启发的，数据给业务的启发越多，业务对数据的需求越\\n\\n多。“手工提数”作为一种“低水平满足”，将由“全民化的数据服务”所取代。 \\n\\n如果期待业务背景的数据用户能够自主的把数据用起来，一定是能够感受到数据对\\n\\n他们的工作带来了帮助和便利，甚至能够直接提升他们工作的成效。能够让用户用\\n\\n数的门槛降下来，让用户用数更加方便快捷，能够快速敏捷地形成数据产出。 \\n\\n通过数据服务一体化平台建设，为各类数据用户提供低门槛的一体化入口，提供浏\\n\\n览、搜推、个性化订制、一键取数和智能交互等服务功能。通过数据服务一体化平\\n\\n台，用户登录时，系统按照客户的角色、工作内容、既往访问行为等信息，推荐给\\n\\n客户最适合使用的数据产品，用户也可以采用主动查询的方式来搜索数据内容，系\\n\\n统将综合考虑资产使用热度、数据源质量等信息对返回结果进行排序，使得用户以\\n\\n最快的方式锁定自己想要访问的数据表。用户可以通过“一键取数”的功能，直接\\n\\n访问数据表中的部分或全部数据内容——而不用按照惯有流程手动进行数据申请并\\n\\n等待审批处理，系统会根据用户数据权限自动审批访问申请。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         二、金融数据发展瓶颈的破题之术  \\n\\n15 \\n\\n智能化服务引擎为业务用户提供更为便利的交互方式——业务人员可以尝试以语音\\n\\n的方式表达自己的数据访问需求，例如：可以问“上个月的个贷贷款总额是多少？”\\n\\n通过对语音和语义的识别，屏幕上会显示《个贷贷款汇总月报表》，供用户浏览和\\n\\n分析。 \\n\\n一站式平台的端到端方案，以可视化、零代码的方式，实现“所想即所见，所见即\\n\\n所得。”让数据的使用更加接地气，真正的走进业务、赋能业务。 \\n\\n4.  严控数据高质提效 \\n\\n“不能翻来覆去做同一件事儿，却期待能有不同的结果。” \\n\\n——爱因斯坦 \\n\\n断言 4：“数据质量决定生存质量。”对未来的数字业务而言，数据质量的好与坏\\n\\n会直接反应在关键经营指标上（比如：坏账率等）。数据质量将建立和食品质量管\\n\\n理一样全流程管控体系，从源头抓起，嵌入数据生产和消费加工全链路各个环节。 \\n\\n在数字化能力建设中，“高质量数据”是保证数据服务效能之“器”，没有“好数\\n\\n据”作为基础，无论在任何数据建设和服务的场景中，都免不了陷入“垃圾进、垃\\n\\n圾出”的无效工作怪圈。 \\n\\n通常来讲，一名成熟的 AI 数据工程师只有 10%-20%的时间真正花在算法编码上，\\n\\n有超过 50%的工作时间，是在对数据进行处理和加工。如果数据质量低，充满空值\\n\\n和错误的数据，就会占用 AI 工程师更多的时间在数据质量的修正和空值处理上。如\\n\\n果 AI 工程师不幸没有识别出全部的错误数据，那么以这些数据作为输入，算法就可\\n\\n能会得出错误的结论，误导决策者做出错误的判断，从而影响业务的发展，这个是\\n\\n更为可怕的后果。 \\n\\n有时数据质量的问题产生是由于一线业务人员缺乏对数据重要性的理解，在数据录\\n\\n入时过于随意，例如因为嫌弃录入麻烦而忽略对重要信息项的填写；有时业务人员\\n\\n会因为各种原因故意录入错误的数据内容，例如：明明应该录入客户真实的手机号\\n\\n码和地址信息，业务员会因为不想把自己的客户信息透露给企业或者其他同事（这\\n\\n种情况在寿险公司非常普遍），就故意把自己的手机号和家庭住址填写到客户信息\\n\\n当中，这种现象严重制约了企业做精准的客群画像，使得企业没办法精准地了解客\\n\\n户，难为客户提供最匹配、最适合的产品。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         二、金融数据发展瓶颈的破题之术  \\n\\n16 \\n\\n数据质量管理的破局思路在于，质量问题必须从源头入手，并贯穿数据生命周期。\\n\\n数据质量的治理卡点必须要镶嵌到数据产生、获取、加工、使用的各个流程，通过\\n\\n数据质量治理工具嵌入到集成、加工、使用的各个环节中，并通过统一的管控平台\\n\\n进行任务分发、任务追踪和整体监控，以此达到对数据质量优化的效能提升。 \\n\\n5.  融合算力数智驱动 \\n\\n“只有精准，才有未来。” \\n\\n——《智能商业》 \\n\\n断言 5：“数据的下一站是智能，数据最终会走向与业务系统的数智融合”，数据\\n\\n消费正在由“人”变成“系统”。未来数据技术将与云原生和智能化全面融合，形\\n\\n成“云数智一体化”服务。 \\n\\n由于历史原因，大部分金融机构数据应用系统都是先于数据平台建设，不同应用涉\\n\\n及的责任主体、流程、数据和系统各自独立，这个阶段应用系统主要以业务需求驱\\n\\n动为主，业务个性化、效率优先，建设方式也是以竖井式的，应用间缺少复用及共\\n\\n享。数据仓库的诞生从组织层面一定程度上解决了复用共享、标准统一的问题，重\\n\\n复建设、各自为政的现象也大为减少。截至目前，在处理策略、语义支持、场景支\\n\\n撑以及工程经验上，数据仓库是目前沉淀下来的企业管理数据较好的解决方案。 \\n\\n当前，大规模数据计算和基于数据的智能决策，已经成为企业业务数据化运营的重\\n\\n要基础。随着大数据系统整体架构趋于稳定，各种引擎的发展逐渐进入收敛期，批\\n\\n计算、流计算、交互分析、机器学习收敛成为四个核心计算模式，变化的趋势不断\\n\\n加快。 \\n\\n1)  数据和算力保持高速增长 \\n\\n未来几年对数据和计算力的需求持续旺盛。同时，海量数据涌入，单位数据的价值\\n\\n持续下降，拿到同样业务效果的算力要求持续上升。因此算力优化会持续成为核心\\n\\n课题，优化会来自于软件本身，软硬结合。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         二、金融数据发展瓶颈的破题之术  \\n\\n17 \\n\\n2)  支持多模态计算和服务化 \\n\\n大数据计算需求场景和模态越来越多样化，覆盖从大规模的离线分析型计算，到交\\n\\n互式查询，到实时数据处理等多种场景。另一方面，由于缺乏能够统一支持多种计\\n\\n算模态的大数据系统，带来了系统部署碎片化、数据冗余、运维成本高等挑战，同\\n\\n时无形中提高了用户使用大数据技术的门槛。未来大数据平台会向整合统一、多种\\n\\n计算模态需求的方向发展，使计算系统更加透明，数据分析作为一种基础能力更加\\n\\n服务化、便捷化，用户的业务逻辑可以做到“一次编写，多场景、多模态适用”。 \\n\\n3)  面向 AI 的大数据系统 \\n\\n当前大数据系统的设计主要面向 BI，随 AI 技术在深度学习上的突破，通过 ML/DL\\n\\n带来更多的业务价值已逐渐被客户认可。大数据平台和机器学习技术的深度融合，\\n\\n包括数据平台传统组件，例如：优化器和索引，基于机器学习算法重新设计，机器\\n\\n学习组件更直接地嵌入大数据平台，直接为数据用户服务，Data Warehouse 和 ML \\n\\nPlatform 的界限越来越模糊。还有一些可能的发展方向包括硬件设计上（包括网络、\\n\\n内存等）更面向 AI 框架优化，从面向表的存储模式转向面向多维的存储，调度和算\\n\\n力分配的重新设计，数据处理充分考虑特征工程的需要，编程接口满足大数据和 AI\\n\\n（以 Python 为主）的融合。 \\n\\n4)  云原生化多引擎融合 \\n\\n云原生化解决方案将充分利用云的资源弹性、异构算力、标准化服务以及容器、自\\n\\n动化、微服务等云原生技术手段，通过弹性和软硬协同优化，持续提升资源利用率，\\n\\n并兼容各类主流或者用户自有的计算引擎，统一运行各类异构工作负载流程，统一\\n\\n管理作业生命周期，统一调度任务工作流，保证任务规模和性能，为 AI/ML 提供工\\n\\n程效率高、成本低、可扩展、可复制的端到端解决方案。 \\n\\n6.  强化数字人才能力 \\n\\n数据分析，分析的从来不是“数据”，而是分析和还原“业务”。 \\n\\n断言 6：未来数据团队与业务团队是双向融合的，大部分业务团队会配备“数据政\\n\\n委”和“数据科学家”，以提高业务方的“数据变现”能力。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                         二、金融数据发展瓶颈的破题之术  \\n\\n18 \\n\\n在上一章节讨论数据服务面临响应慢的困惑中，描述了业务需求和技术实现上的差\\n\\n异问题，其根本的破局关键在于真正“打通”需求和技术的“双向不理解”，洞悉\\n\\n应用场景侧真正的诉求，选择最适合的技术架构和原型方案，以数据产品的形式落\\n\\n地，并通过产品运营工作持续优化和迭代，从而优化开发效能，改善开发服务供给\\n\\n能力，在这些方面，人的因素至关重要。数据产品经理或将成为数据创造业务价值\\n\\n的“破局者”。 \\n\\n1)  数据产品经理是谁？ \\n\\n数据产品经理，以“业务赋能”为第一目标，引领各部门和角色充分协作，以“数\\n\\n据产品”为手段和载体，最终实现数据对一线业务能力的促进和提升。数据产品经\\n\\n理不是简单的需求“承接方”，而是主动打破当前的问题解决思路，通过对业务的\\n\\n深入洞察，重构解决方案，将贯穿数据产品生命周期的始终，从需求形成阶段就参\\n\\n与和陪伴业务用户，从需求分析阶段开始、理解业务诉求、洞察和判断产品的应用\\n\\n价值、主导产品的规划设计、推进产品开发、并在产品上线后了解用户反馈、持续\\n\\n优化并推广产品的用户范围。 \\n\\n2)  数据产品经理需要具备什么条件？ \\n\\n称职的数据产品经理是懂业务、懂数据、懂产品、懂技术的多重能力素养的人才，\\n\\n能够熟悉业务流程，熟知现有数据服务情况，并能够分析其中可能的改善点；理解\\n\\n业务所沉淀的数据，并了解业务对数据的需求；掌握产品设计的基础技能；例如：\\n\\n数据产品设计方法和标准工作流程、设计工具等；具备对数据以及主流数据技术的\\n\\n理解，例如：数据的存储形态、AI 和 BI 的主流技术等；此外，还要具备开阔的视野\\n\\n并充分实践精神。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n19 \\n\\n三、金融数据能力建设的核心观点 \\n\\n数据能力是一面业务“镜子”，如何把数据的价值从“后视镜”（做历史分析），\\n\\n转变为“望远镜”（看方向）、“放大镜”（精细化操作）、“显微镜”（发现问\\n\\n题真相），关键是看企业的数据体系的完整度（木桶的短板）和数据能力的应用程\\n\\n度（水位的高低）。 \\n\\n数据能力和数据体系就像“水”和“木桶”的关系，水位越高、要求的木桶短板越\\n\\n高。为了具备更高的数据能力的“水位”，则需要数据体系“木桶”的每块板越长。\\n\\n我们认为，数据体系需要从顶层设计、业务价值、数据服务、数据治理、数智算力、\\n\\n数字人才等 6 个维度“木板”来整体考虑和建设，避免长板够长、但短板漏水的情\\n\\n况。 \\n\\n1.  以全局视角的驱动力 \\n\\n“公司级数据能力体系，需要站在整体看局部、站在结果来看过程。” \\n\\n传统的数据架构就像“老城”，是自然生长出来的。而现代的数据架构就像“新城”，\\n\\n是“先规划再建设”的，首先需要一个整体数据能力蓝图，围绕企业数据战略和对\\n\\n应的数据能力要求（从顶向下），系统化落地“3+1”数据体系（数字基础设施、数\\n\\n据资产、数据应用、运营机制）（从底向上），并实现提升效率、降低成本、提高\\n\\n质量、敏捷创新、人才培养和生态建设的 6 大核心能力建设目标。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n20 \\n\\n1)  蓝图规划（自顶向下） \\n\\n通过自顶向下的设计方法，整体分析企业数据现状，拆解当前数据能力，归集并总\\n\\n结存在问题，结合企业数据战略，统一规划和设计数据体系的具体目标和落地路径，\\n\\n推动企业整体数字化经营，从而体系化解决原有数据体系分散建设的问题。 \\n\\n2)  “3+1”数据体系（自底向上） \\n\\n随着数据规模化发展，数据领域从技术架构、数据架构、数据研发、数据管理以及\\n\\n数据应用都提出新的要求，由此催生构建数据体系的算力、数据和价值的三大驱动\\n\\n力和一套运营保障机制。 \\n\\n驱动力一：算力驱动，打造企业级数字基建。 \\n\\n所谓数据基建是为了满足大规模离线计算、实时计算、AI 等各类数据处理引擎的算\\n\\n力需求，提升数据算力资源供给效率，从而利用云原生、容器技术，采用存算分离、\\n\\n多引擎算力融合、统一调度等技术构建的“云原生化数据底座”，具备如下 6 大特\\n\\n点： \\n\\n•  数字基建使用基于云的基础架构来支持资源管理、可扩展性以及运营效率。 \\n\\n•  通过容器化技术在资源隔离和提供一致性的开发、测试和运维环境。 \\n\\n•  利用多租户和安全技术，为用户提供一套综合的数据安全环境，解决数据安全\\n\\n共享问题。 \\n\\n•  采用计算、存储物理分离，实现计算层与存储层解耦。 \\n\\n•  通过构建统一数据库湖，计算层通过容器技术和数据沙箱，实现不同算力模型\\n\\n的快速交付和稳定运营。 \\n\\n•  利用云和容器化的能力，集成实时、离线、流式数据存储与计算产品，关系型数\\n\\n据库、MPP 数据库等，多引擎支撑多种不同的数据计算与分析需求，同时实现\\n\\n数据在多计算引擎间进行融合，减少数据搬迁和存储，消除不同引擎数据研发\\n\\n和模型的割裂问题，提升资源效率，降低成本。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n21 \\n\\n驱动力二：数据驱动，沉淀企业高价值数字资产。 \\n\\n所谓数字资产是指企业数据的核心资产，包括数据表、数据模型、指标、标签等。\\n\\n通过“采、建、管、用”的数据体系建设方法和“盘、评、治、享”的数据资产管\\n\\n理方法，可以分为多层的数据资产管理： \\n\\n针对数据模型层：数字资产建设通过统一的企业数据公共层建设，构建企业数据视\\n\\n图，帮助数据管理者提升数据资产质量和资产运营能力。 \\n\\n针对数据研发层：通过建设一体化数据研发平台，建立数据集成、数据建模、数据\\n\\n研发、数据分析、运维管理等的一站式能力，同时数据研发需要同时支持实时、离\\n\\n线研发、AI 模型训练及研发等能力，提高数据研发效率，加速高价值数据的资产化。 \\n\\n针对数据治理层：通过构建全链路数据治理和企业级数据标准，实现包括数据架构、\\n\\n资产目录、数据质量、数据安全、元数据等在线管理能力，消除线上线下两张皮的\\n\\n现象，提升数据资产质量，保障资产健康度，并利用一系列生产力工具（例如：数\\n\\n据产品、BI、AI）等构建自助式的数据服务，为业务场景提供敏捷的服务体验。 \\n\\n驱动力三：价值驱动，业务场景驱动的数字应用。 \\n\\n所谓数字应用是指通过沉淀的核心数字资产，数字应用可以通过多种形态的数据服\\n\\n务方式满足多场景数据消费的需求： \\n\\n•  如 API 服务可嵌入业务流程使用。 \\n\\n•  数据产品可直接为客户经理提供展业服务。 \\n\\n•  BI 和 AI 工具可帮助分析师进行在线分析和挖掘数据。 \\n\\n•  数字门户还可通过集成方式打造一站式数据工作台。让数据应用真正做到 “水\\n\\n到田头”多元化服务和便捷的数据消费体验。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n22 \\n\\n一套机制：运营机制，组织及运营的持续保障。 \\n\\n运营机制是一个企业的“数据软实力”，是保障数据体系能够长治久安、持续发展\\n\\n的“数据军规”。 \\n\\n它本身就是需要基于数据驱动的“机制”，而非“人治”。基于数据平台的技术、\\n\\n业务、管理、运维等多维元数据信息，结合业务目标和有限的资源（技术和人力），\\n\\n建立一套匹配的资产运营目标（资产复用率、业务覆盖率、降低成本等）、资产管\\n\\n理制度（发布变更、上下架、安全管理、资产积分）等，同时建立组织保障（数据\\n\\n产品经理、数据认证、创新大赛等）和数字化人才成长体系，在企业内部形成数据\\n\\n素养的文化氛围，通过不断学习和运用，让使用数据成为日常工作中不可或缺的内\\n\\n容。并不断完善全链路治理跟踪机制，不断进行资产评估和价值分析，打造企业的\\n\\n“数据精品品牌”心智。通过运营保障机制的不断作用，让数据真正成为推动企业\\n\\n高质量发展的“核心动能”。 \\n\\n3)  “6”大核心能力 \\n\\n通过“3+1”数据体系的建设，将帮助金融机构在以下 6 项核心能力上得以提升： \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n23 \\n\\n2.  以分层视角的核心力 \\n\\n分层视角是全局视角的进一步细化和落地指导，帮助我们将复杂问题简单化，化整\\n\\n为零，分层分类有序推进。 \\n\\n在数据能力体系总体框架基础上，我们采用自下而上的设计方法，更细粒度拆解和\\n\\n抽象“3+1”数据体系，从支撑海量计算和存储能力的数字基建，围绕采建管用的数\\n\\n字资产，以数据驱动业务经营的数字应用，以组织保障的运营机制四个维度详细阐\\n\\n述数据体系的核心能力。 \\n\\n1)  数字基建 5 大法则 \\n\\n针对海量数据算不动、效率低、成本高、难运维等方面的问题，新一代大数据平台\\n\\n建设通常遵循以下 5 大法则： \\n\\n法则 1：具有云原生可扩展的多种计算模式融合 \\n\\n云原生框架天生具备快速交付、弹性伸缩、标准化、自动化、隔离性等诸多优势，\\n\\n持续提升资源利用率。再结合云原生数据架构的存算分离、融合计算、混合部署等\\n\\n优势，支持批、流、交互式、多模、图等不同计算模式的融合，例如：湖仓一体、\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n24 \\n\\n流批一体、流式机器学习，使多种计算系统进行深度整合，在功能、生态上形成互\\n\\n补，用户能够在一套系统内完成更多种类型计算,提升平台运行效率，降低使用成本。 \\n\\n法则 2：具有多层智能化的分布式存储层 \\n\\n存储计算分离会在两三年内成为标准，数据平台向托管化和云原生的方向发展。存\\n\\n储内部精细化的分层成为平衡性能和成本的关键手段，基于分布式存储系统上的多\\n\\n层存储（热存储/标准存储/冷存储等）与存储利用率相结合实现存储降本。其中，\\n\\nAI 在分层算法上将发挥更大的作用，编码和压缩在通用处理器上的优化空间有限的\\n\\n情况下，未来更大的突破和技术换代将取决于软硬一体化的技术发展及应用情况。 \\n\\n法则 3：具有统一调度和弹性伸缩的资源池管理 \\n\\n随着数据湖存算分离不断深入,  围绕基于云原生架构下来建立统一容器化资源调度\\n\\n系统成为数据湖存算分离发展的必要组件，为大数据与 AI 一体化架构提供统一资源\\n\\n池化及在离线混部的基础支撑；通过统一算力资源池实现资源统筹调度，优化资源\\n\\n细粒度的管理与调度，可以将离线计算与其它在线计算任务进行资源混部达到峰谷\\n\\n互补的效果，有助于提升服务器资源利用率；同时，也可以根据业务优先级分配计\\n\\n算任务资源，确保资源调度期间不发生争抢，实现在业务高峰期，以弹性扩缩容模\\n\\n式调用算力资源，充分发挥资源算力，提升响应效率。 \\n\\n法则 4：具有异构计算引擎的工作负载协同 \\n\\n从计算引擎层面看，多种异构引擎混部协同是提升大数据资源利用率的重要手段之\\n\\n一，主要包括“离线实时一体、湖仓一体、大数据 AI 一体”等。其中： \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n25 \\n\\n•  离线实时一体 \\n\\n单纯的离线或实时架构都无法完全满足业务场景的需求，通过在两类系统中进行数\\n\\n据同步来支撑不同的场景需求，也对整体数据链路的成本、代价和复杂性带来了不\\n\\n小的挑战。通过提供面向异构计算工作资源负载下的统一多维度查询分析服务，在\\n\\n线与离线计算共用计算和存储资源，解决资源波峰波谷问题，实现资源动态削峰填\\n\\n谷。另外，近实时架构兴起，在离线批量计算和流式实时计算之间，近实时架构避\\n\\n免了流计算庞大的状态存储与管理，在成本和延迟上找到了另一个平衡。随着近实\\n\\n时架构的逐步形成，计算架构最终将实现从离线到实时的全面支持能力。 \\n\\n•  湖仓一体 \\n\\n湖仓一体是一种结合了数据湖和数据仓库优势的新范式，解决了数据湖的局限性。\\n\\n在用于数据湖的低成本存储上实现与数据仓库中类似的数据结构和数据管理功能。\\n\\n提供数据管理特性和高效访问性能，支持多样数据分析和计算，综合了数据仓库以\\n\\n及数据湖的优点形成了新的架构。 \\n\\n•  大数据 AI 一体 \\n\\n支撑规模化的业务增长仍是数据平台需要解决的问题。当前，AI 计算与智能数仓融\\n\\n合、算力加速的需求已经成为趋势，因此数据平台在智能化、多计算模式融合、软\\n\\n硬一体化架构升级、以及数据管理智能等方面的发展正在进入到加速期，未来数据\\n\\n即智能。数据与 AI 一体对平台能力提出了相应要求，云原生化的算力与数据、规模\\n\\n化的调度与编程范式、标准化和普惠化的开发与服务、一体化的服务平台让 AI 工程\\n\\n化具备基础能力。大数据与 AI 应用一体化，把 AI 技术通过数据赋能给业务，成为\\n\\n数据智能的新基石，让数据分析、治理和智能应用变得更简单。 \\n\\n法则 5：具有大数据 SRE 智能运维能力 \\n\\n大数据技术多样性和数据平台架构的复杂性，为大数据平台的运维带来挑战。新一\\n\\n代大数据平台可支持在线滚动升级，缩短升级时长；提供统一运行各类异构工作负\\n\\n载流程，统一管理作业生命周期，统一调度任务工作流，为任务的规模和性能提供\\n\\n保证，通过作业日志，性能指标，资源利用率等数据，结合历史记录和实时负载情\\n\\n况，使用机器学习方式进行分析、检测和调优，在查询计划、数据模型、资源管理\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n26 \\n\\n自适应，以及系统异常检测和自愈等方面不断优化，形成大规模数据平台的智能化\\n\\n运维能力。 \\n\\n2)  数字资产 8 项能力 \\n\\n“采、建、管、用”打造好数据，“盘、评、治、享”管理好资产。 \\n\\n数据资产是数据对业务赋能的核心能力层。它通过全域数据资产建设、智能化数据\\n\\n分析手段以及多样数据服务能力，实现全企业数据的采集、建设、管理、应用和反\\n\\n馈的闭环，打通各层级与多业务间的数据壁垒，实现数据的统一整合与运营，提高\\n\\n企业数据建设和应用的效率。下面我们依然通过采、建、管、用四个关键能力介绍\\n\\n数字资产的构建、管理和运营方法。 \\n\\na)  采：企业数据资产全域采集 \\n\\n“采”的核心能力在于“全域和时效”。全域是解决企业数据完整性问题，时效是\\n\\n解决数据实时性问题，同时多样数据源为了确保与企业内部数据融合，还需要规范\\n\\n化和标准化的管理机制保障。全域数据采集包括按照数据采集的频度、采集数据内\\n\\n容以及企业内外部数据的采集和获取方式进行划分。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n27 \\n\\n从数据采集频度划分，一般分为批量数据采集和实时数据采集，批量数据定义的采\\n\\n集周期通常为小时、日、周等。实时数据一般包括在批量采集中，采集周期通常为\\n\\n分钟；实时数据的采集周期是以秒为单位，数据产生后立即采集。随着经营管理决\\n\\n策对时效性的要求越来越高，金融机构对实时数据的采集需求也随之增多，对企业\\n\\n级的实时计算提出了更高的要求。 \\n\\n从采集数据内容的角度，一般分为业务数据、日志数据、行为数据等内容。业务数\\n\\n据和日志数据通常由后端业务系统产生，以数据库或文件形式存储，可通过离线和\\n\\n实时采集工具进行采集。而对于金融机构的客户行为数据，通常需要移动端、PC 端\\n\\n等系统的埋点技术进行采集。 \\n\\n埋点采集是采用可视化埋点技术，通过标准化业务模型，即应用 -方案-页面-事件-\\n\\n属性，进行线上系统管理，形成统一的埋点元数据。通过线上全流程管理需求-开发\\n\\n-测试-上线，保障埋点数据质量，结合可视化分析工具，沉淀流量数据资产。后续\\n\\n结合数据连接技术形成客户的行为数据资产，可用于客户经营旅程分析。目前完整、\\n\\n准确、易用的行为数据是金融机构相对比较缺失的数据资产。 \\n\\n从企业内外部数据采集的视角，内部数据可通过相应的离线或实时采集工具进行，\\n\\n外部数据对接一般采用数据 API 的方式。随着数据安全合规要求的提升以及跨企业\\n\\n间数据连接价值的挖掘，隐私计算技术逐渐被金融机构采用，让不同机构的数据可\\n\\n用不可见，解决机构间数据协同计算过程中的数据安全和隐私保护问题，推动机构\\n\\n间数据流通和共享。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n28 \\n\\nb)  建：敏捷构建企业级核心数据资产 \\n\\n“建”的核心能力在于“敏捷和复用”。敏捷解决的是数据模型研发效率低的问题，\\n\\n复用解决的是公共层、萃取层建设的企业级数据资产共享问题，建设“好”资产是\\n\\n企业提升数据能力的基础。 \\n\\n数据资产的建设需要从企业级视角规划设计，同时结合应用场景按照分层逻辑进行\\n\\n数据加工和处理，并针对多个部门场景的分析，逐步沉淀为企业具有复用能力的数\\n\\n据资产，减少不同业务部门为了满足数据需求，单独建设数据，形成更多数据孤岛\\n\\n等问题。此外，通过数据资产的共享打通原有部门间的数据壁垒，构建了企业内数\\n\\n据互通的底层逻辑，让数据为企业数字化释放更多的活力。 \\n\\n整个数据建设体系较为复杂，本文从数据模型体系、指标/标签体系、数据连接技术、\\n\\nDataOps 敏捷研发等方面介绍如何打造企业级“好”资产。 \\n\\n•  数据模型分层体系 \\n\\n数据模型分层体系构建的是企业数据能力的底层逻辑。 \\n\\n简单回顾一下数据模型分层设计，通常分为四层，即贴源层、公共层、萃取层和应\\n\\n用层。 \\n\\n贴源层总体上保持与源系统一致，可轻度进行数据清洗。数据应用原则上不应从贴\\n\\n源层直接提取数据。但目前从行业平均情况看，有近 50%的比例是直接到贴源层使\\n\\n用数据，这是造成大量数据重复存储、数据孤岛的主要原因。 \\n\\n公共层之上是我们重点建设的内容，公共层数据模型需要从企业视角梳理和汇总各\\n\\n个业务部门用数需求，从业务场景入手提取数据共性并设计相应数据模型，最终通\\n\\n过设计即开发的建模技术，快速沉淀为公共层模型资产，是作为业务部门数据应用\\n\\n的主要来源。 \\n\\n萃取层是基于公共层数据模型构建的企业级指标体系和标签体系，具体建设方法可\\n\\n参考后续章节。 \\n\\n应用层是基于公共层和萃取层建设，为不同业务条线的需求提供个性化数据服务的\\n\\n能力，也是常说的集市层。应用层模型可根据企业自身数据团队和业务分析团队定\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n29 \\n\\n位确定建设主体，通常采用共建模式，数据团队提供计算和存储资源、数据标准、\\n\\n研发工具等，而业务分析团队负责与其业务相关的数据加工、指标和标签建设，通\\n\\n过组织协同机制，拉近数据团队与业务团队的距离，真正为企业打造百花齐放的数\\n\\n据应用场景。 \\n\\n•  数据模型设计即开发 \\n\\n模型设计即开发是数据研发高质高效的利器。 \\n\\n从模型设计方法上，包括三范式模型和维度模型。传统数仓通常采用三范式的建模\\n\\n方法，已经在金融行业形成了一定的标准与模型体系。三范式建模的优点是减少数\\n\\n据冗余，物理存储层面尽可能降低成本。缺点是业务分析需求与三范式模型体系无\\n\\n法直接映射，需要进行特殊处理，造成模型开发效率低。此外，查询效率方面，在\\n\\n数据量小有索引情况下表现不明显，但如果数据量数以亿计，查询速度十分缓慢，\\n\\n甚至可能会造成宕机，因此三范式模型不适合直接应用于业务决策分析和大规模计\\n\\n算场景。 \\n\\n维度模型是 Ralph  Kimball  在 90 年代提出的数仓建模理论，它从企业决策分析角\\n\\n度出发，旨在使用户更快的完成数据分析和大规模复杂查询的数据分析服务。从模\\n\\n型设计方法上更贴近业务和数据分析人员理解的角度，因此维度建模也成为金融行\\n\\n业构建数据模型的重要选择。当然不同金融机构需要结合自身实际情况，选择适合\\n\\n的建设方式，有些基于贴源层直接用维度建模构建公共层模型，这种方式适合尚未\\n\\n建设数仓或现有数仓不满足需求需要重构的企业。有些在已有三范式模型的基础上，\\n\\n构建维度模型层支持应用层，这种方式适合数据体系相对完备，并且三范式模型能\\n\\n够覆盖大部分应用场景的企业。 \\n\\n不论是范式模型还是维度模型设计，建模效率则成为敏捷数据研发体系中的重要环\\n\\n节。利用建模工具，以设计即开发的理念实现数据建模与数据研发联动。在建模过\\n\\n程中，关键是确保设计原则、标准及规范的真正落地，而不再是线下的管理要求和\\n\\n约束，如数据域定义、业务过程拆解原则、指标定义规范等，通过建模工具进行标\\n\\n准贯标，并与数据开发深度融合，基于概念模型快速创建逻辑模型，采用智能数据\\n\\n研发引擎根据逻辑模型自动生成相应的物理表及调度任务，因此通过逻辑模型屏蔽\\n\\n了底层物理表的细节，数据分析人员直接访问逻辑模型即可拿到最终结果。通过设\\n\\n计即开发的智能建模工具不仅解决了原有数据研发效率低的问题，也有效地从源头\\n\\n把控标准，提升数据研发质量，为数据治理奠定基础。 \\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n30 \\n\\n•  维度模型构建数据公共层 \\n\\n数据公共层的建设不是一蹴而就，是由局部逐步沉淀而成。 \\n\\n那么如何来构建数据公共层？如上所说，在数据仓库领域，数据建模一直对范式建\\n\\n模还是维度建模有两种争锋相对的观点。而现在我们在大数据应用的场景，一般只\\n\\n提一种方法，就是维度建模。 \\n\\n通过前文，我们已经了解维度模型的构建是以实际业务需求为导向，模型也是通过\\n\\n不断的需求累积出来，并需要适应快速的业务变化。那么公共层数据模型不是一开\\n\\n始进行企业级设计就能够全部覆盖的，也绝非一蹴而就，通常是由业务场景驱动，\\n\\n由局部的业务需求演化为大量共性需求，再使用维度建模的方法构建而成。数据公\\n\\n共层建设是多种场景业务需求的一个复合，代表了企业数据能力最基础和最通用的\\n\\n模型。核心解决了企业级数据的全局一致性、可复用、可共享的问题。 \\n\\n好的数仓公共层数据模型设计，需要满足以下几方面的目标：可用与易用性，性能\\n\\n与成本，质量与效率，稳定与扩展性。在实际建设过程中，可结合企业自身特点，\\n\\n包括数据量、资源、业务需求等因素选择侧重目标。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n31 \\n\\n•  建设企业级指标/标签体系 \\n\\n从层次上来说，数据公共层至关重要，起到承上启下的作用，向下负责跟上游多个\\n\\n交易型业务系统对接，向上对应用集市屏蔽了上游变化带来的影响，使得应用层只\\n\\n需关注利用公共层的模型解决自己的业务需求，比如指标、标签、产品等。 \\n\\n首先，指标是企业重要的高价值数据资产。是数据萃取层主要建设内容。指标是业\\n\\n务信息的“高浓度”萃取，每个指标加工结果，都是对业务范围的清晰理解和对加\\n\\n工口径的准确加工。指标体系的构建要以对业务目标和业务过程的理解为前提，以\\n\\n完整反映业务经营状况为目标，从整体到局部，逐层提炼和分解得出。指标体系建\\n\\n设的两步原则： \\n\\n一是业务分析：指标体系的构建过程由业务分析开始。首先分析企业的业务板块，\\n\\n定义数据域。再基于具体业务过程的分析，定义维度属性。 \\n\\n二是指标构建：指标定义的范围包括原子指标定义和衍生指标定义。原子指标是基\\n\\n于某一业务行为事件的度量，其特点是不可再分拆，例如：贷款余额；派生指标由\\n\\n“原子指标+修饰词+维度”组成，是对原子指标在业务统计范围周期内的统计值。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n32 \\n\\n基于以上原则，在业务过程的分析，建立业务过程和维度的总线矩阵关系，确保在\\n\\n数仓中公共维度不重复，在不同业务过程中保持一致，并对不同系统间的维度进行\\n\\n逻辑和概念上的统一和标准化。然后进行原子指标的定义，实现全局指标口径唯一，\\n\\n解决指标口径二义性的问题。 \\n\\n通过一定组合关系生成派生指标，从而形成统一的指标体系，同时通过工具平台的\\n\\n规则配置，从技术层面保证相同业务和技术涵义的指标有且只有一个，避免“同名\\n\\n不同义”、“同义不同源”的乱象，确保业务在使用指标时的准确性。 \\n\\n其次，标签也是业务价值挖掘的重要数据资产。标签是面向业务的数据资产组织方\\n\\n式，实现数据资产对于业务可见、可懂、可用，通过标签体系构建，标签场景化拆\\n\\n分，打通数据资产和业务的通道，解决数据人员和业务人员“鸡同鸭讲”困境，发\\n\\n挥数据价值，让数据“用”起来。 \\n\\n标签体系构建，核心打造围绕“金融产品、内容、渠道”等多对象实体的标签体系，\\n\\n有效支撑面向总部运营人员的渠道投放、私域运营等场景，面向一线理财经理的客\\n\\n户经营、精准营销等场景，有效提升用户终端体验和机构服务效率。 \\n\\n企业级标签体系的建设以业务经营场景为驱动，基于“统一设计、统一建设、统一\\n\\n完善、统一运营”的四个统一原则，全面进行规划，实现标签全生命周期线上化、\\n\\n平台化、透明化。构建标签体系需要遵循一下四项原则： \\n\\n一是理现状：梳理当前业务状态，发现技术和业务短板，结合业务需要和业务现状\\n\\n设计标签体系，同时设计相应的管理制度和流程。 \\n\\n二是引实践：引入金融行业指标、标签体系建设的实践经验，规划标签体系。 \\n\\n三是重场景：深入标签应用各类场景，对接现有数据和系统，契合业务需求完成功\\n\\n能建设，为数字化客户服务、数据分析、业务决策提供有力服务。 \\n\\n四是奠基础：满足部门诉求的同时，为企业级应用与推广奠定基础，结合标签统一\\n\\n管理的制度与流程，提供运营辅导，促进业务目标的实现。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n33 \\n\\n•  构建数据连接 \\n\\n围绕企业内部数据，在数据合规的前提下，如何识别来自不同设备、不同注册账号、\\n\\n不同身份的同一个客户呢？通常我们可以通过数据连接技术实现多端设备连接，建\\n\\n立数据互通。 \\n\\n“数据连接技术”具体指什么？是金融机构常说的客户编号吗？客户编号是已经和\\n\\n金融机构发生交易往来产生的，还有大量的未知客户，如游客通过营销渠道下载手\\n\\n机银行准备注册但中途放弃，又或者一个金融机构由于不同部门建设不同 APP，导\\n\\n致客户注册信息不同，难以从客户为中心的视角全面分析其行为和偏好，因此需要\\n\\n通过智能化的技术手段将不同端的数据形成映射，建立数据连接。 \\n\\n数据连接技术可以生成代表自然人的身份代号，类似于实际生活中的身份证号，是\\n\\n通过算法赋予的一个稳定的虚拟身份 ID，并且识别自然人所拥有的各类身份 ID 和\\n\\n工具 ID，如账号 ID、设备 ID 等。数据连接还将自然人来源各个系统、各个领域的\\n\\nID 进行聚合，突出活跃 ID，剔除不属于该自然人的 ID，为精准营销、标签体系建设\\n\\n等提供帮助。如下图所示： \\n\\n同样，企业客户通过知识图谱构建围绕企业基础信息、经营行为、风险能力、资产\\n\\n资质、资讯环境、关系网络等领域的企业数据资产，并基于企业关系图谱的深度挖\\n\\n掘推动在风控、招商、商业化等领域的业务发展。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n34 \\n\\n显而易见，因为无论个人客户还是企业客户，其信息是散落在各个场景中的，要想\\n\\n了解客户需求，就需要构建全面客户画像。例如：某科技公司在银行开有账户，过\\n\\n去银行仅仅通过账户流水掌握其经营特征。 \\n\\n现在可以通过数据连接体系，让银行归集相关数据后可清晰刻画出该企业的真实经\\n\\n营情况，如这家企业由于实现技术突破潜在订单趋于旺盛。由于企业扩大规模产生\\n\\n大量结算、贷款、甚至上市等需求，还会增加销售回款带来的存款、理财需求。此\\n\\n外，银行还可以通过经营流水反查其产业链上下游往来的商业伙伴，建立“客户关\\n\\n系圈”图谱，可进一步挖掘大量的潜客线索。 \\n\\n通过这个场景可以看到，数据连接技术是通过“数据+算法+服务”（如下图）技术\\n\\n实现原有孤岛数据的连接，通过对客户的多维度刻画，让数据与业务场景的连接更\\n\\n加紧密，最大程度挖掘数据价值。 \\n\\n•  数据研发运营一体化（DataOps） \\n\\n数据研发领域 DataOps 效率提升的目标就是“更快的交付高质量的数据”。本质\\n\\n是通过贯穿数据研发的全生命周期，覆盖事前、事中、事后，从而满足数字化转型\\n\\n不同阶段、不同层次数据交付的需求。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n35 \\n\\nDataOps 是面向数据全生命周期，包括从数据需求输入到交付结果输出到全链路数\\n\\n据研发过程，在系统工具、组织模式、安全风险管理的支撑下，实现数据研发运营\\n\\n一体化、敏捷化、标准化、自动化、智能化、价值化的理念。一个成功的 DataOps\\n\\n具有如下 5 大特质，其中： \\n\\n持续性：首要保证的就是尽可能的持续性，不间断，不论什么样的情况出现，都能\\n\\n够自适应的持续让数据流动起来，所以持续性是 DataOps 的首要特质。 \\n\\n敏捷性：在持续的基础上，DataOps 需要一定的敏捷性，能够快速响应外部的各种\\n\\n变化，支持多种云化部署模式，公共云、专有云、虚拟机、容器等；自动支持数据\\n\\n湖和数据仓库；支持未来的架构变化。 \\n\\n全面性：作为企业全域数据的底座，DataOps 要全面的支持所有的场景和数据。 \\n\\n可信：数据的可信非常重要，保证数据资产和用户产生的数据集的数据目录可访问\\n\\n性；能够清晰的知道数据从哪里来的，是怎么被加工和处理的数据血缘；确保每一\\n\\n个源数据在变化的时候所有相关的数据集也被复制和更新的数据验证。 \\n\\n自动化：自动化是 DataOps 的重要基础能力，从数据的产生，处理到交付数据产品\\n\\n和服务，整个过程要尽可能的自动化处理。 \\n\\nDataOps 为数据体系构建带来的价值主要包括，提供实时的数据洞察能力、加速数\\n\\n据应用的构建过程、让数据价值链的每一个角色都能更高效的协作、提供数据的透\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n36 \\n\\n明度，从而能够更好地产生数据创新和增进协作、提升数据和数据服务的可复用性、\\n\\n优化数据质量、构建统一、标准化的数据协作能力。 \\n\\nc)  管：打造精品数据资产管理及运营体系 \\n\\n数据体系建设的“采、建、管、用”四个环节中，“采”和“建”的工作目标是完\\n\\n成数据获取和公共数据资产的沉淀，而“用”的目标是达成应用侧的场景化赋能，\\n\\n那么“管”则是“建”和“用”之间的“中腰”环节。数据资产管理和运营的核心\\n\\n目标，是对企业拥有的数据资产有全面完整的了解和充分的管控，就像球场上的中\\n\\n锋队员，观察全局，精准筛选数据资产，把业务侧需要的数据服务“精准、稳定、\\n\\n快速”的“传球”到业务具体场景的先锋队员脚下，助力实现业务  “临门一脚”。 \\n\\n如上图，我们将完整的数据资产管理能力总结为：“盘”“评”“治”“享”4 项\\n\\n扩展能力（即数据资产盘点、数据资产评估、数据资产治理和数据资产服务），具\\n\\n体内容如下： \\n\\n•  “盘”：全面了解家底。 \\n\\n数据资产盘点工作主要是在明确数据资产范围的基础上，构建数据资产目录，盘点\\n\\n数据资产内容、存储和管理情况，并通过可视化手段支持各类数据用户快捷高效的\\n\\n查询和了解数据资产情况，摸清底数。数据资产盘点工作的目标不应该只拘泥于解\\n\\n决“有什么数据、数据在哪里”的问题，而是要通过盘点的动作，解决对数据的一\\n\\n致性理解问题——这里讲的“理解”，既包括对数据内容的理解、同时也包括对数\\n\\n据权利和数据责任的理解。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n37 \\n\\n对数据资产内容的理解，首先，需要通过构建统一数据资产类目和数据资产挂载实\\n\\n现对数据的业务内容以及业务分类的统一理解；其次，通过多维度的数据资产标签\\n\\n体系建设，丰富对数据的描述和不同应用场景当中的灵活应用。 \\n\\n对数据权利和责任的理解，则需要通过数据资产管理和确信机制的建立来实现。数\\n\\n据资产的管理和确信机制是在数据资产盘点的活动过程当中逐渐确立的。例如，数\\n\\n据类目体系建设的过程中，会征求业务相关部门从业务视角审视和复核数据分类的\\n\\n合理性；在数据资产标签体系建设过程当中，会征求开发部门就僵尸资产、过长链\\n\\n路资产等进行技术阈值定义。通过工作上的协同，各部门的数据工作角色、权力和\\n\\n责任划分逐渐清晰，并沉淀在相关的管理机制和流程当中。 \\n\\n特别需要强调的是，由于金融机构已经沉淀了海量数据资产，那么资产盘点的效率\\n\\n也是必须考虑的重要因素。通过实践，采用知识图谱、自然语言等人工智能技术手\\n\\n段，可实现数据资产盘点效能的指数级提升。 \\n\\n•  “评”：评估资产价值。 \\n\\n数据资产评估针对资产价值、资产效能、资产质量、资产活性和资产安全等不同维\\n\\n度建立对数据资产的观察和评价方法，以量化的方式描述和评价数据资产，便于数\\n\\n据生产者、数据管理者和数据使用者可以更准确的了解数据资产的状态。下面是数\\n\\n据资产评估不同维度的具体定义或方法，可作为实践参考： \\n\\n价值评估。通常指数据资产价值的货币化度量，即数据资产的价值。例如我们购买\\n\\n外部数据花了多少钱，这就是数据资产的成本价值。或者我们依据算法产生的营销\\n\\n清单使理财产品销售额提升了 100%，这就属于数据资产创造的业务价值。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n38 \\n\\n质量评估。即评估数据资产的数据质量。评估维度通常包括数据完整性、准确性、\\n\\n有效性、一致性、唯一性等。数据资产质量是数据可用性的重点评估维度，数据质\\n\\n量差也是目前金融机构数据领域普遍面临的问题。 \\n\\n活性评估。即对数据资产的维护和使用的频繁程度进行评估。数据资产使用频率意\\n\\n味着数据是否充分被复用。长期未被使用，可定义为僵尸资产，考虑进行资源降配\\n\\n或者删除。 \\n\\n资源评估。即量化数据资产的资源使用成本，包括计算资源、存储资源等。数据资\\n\\n产资源评估是实现企业数据资产成本核算的基础，让金融机构逐步建立数据成本的\\n\\n计量计费体系，促进企业持续增效降本。 \\n\\n安全评估。金融行业数据资产安全是一道不可触碰的红线，数据的有效应用必须以\\n\\n满足金融行业监管机构发布的各类行业安全规范和监管要求，以及金融企业内部的\\n\\n数据安全管理要求为前提。 \\n\\n综上，数据资产评估的视角非常丰富，在具体工作中，到底优先从哪个视角展开评\\n\\n估、评估工作的方法和颗粒度应该如何把握？这些问题都要先深入明确数据资产的\\n\\n评估目标，如降本增效，则需要进行资源使用成本评估等，在确定评估目标后，再\\n\\n选择相匹配的评估视角和工作方法。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n39 \\n\\n•  “治”：提升数据质量。 \\n\\n数据资产治理是从数据资产可用性的角度提升数据质量。新一代数据治理机制的关\\n\\n键点在于“全链路数据治理”，抓住数据生产源头，强化业务系统元数据管理，从\\n\\n源头消除数据标准不落地问题，推动治理工作迁移，将规范设计嵌入研发体系，形\\n\\n成全链路数据治理能力，从而解决设计与运行、线下与线上两张皮的现象。 \\n\\n传统数据治理往往以分析类系统（即 AP 系统，典型系统为数据平台类、数据集市\\n\\n和应用系统为代表）为主要治理对象，很少从数据生产源端的业务系统入手，没有\\n\\n在根源上杜绝问题的发生，所以各个金融机构常常发动数据治理“运动”，但数据\\n\\n质量依然不尽人意。传统数据治理是治“表”。 \\n\\n全链路数据治理是覆盖数据生产、数据分析以及数据消费全链路，首先是以全域数\\n\\n据为治理对象，既覆盖生产域（TP 环境，业务系统数据库），也覆盖分析域（AP 环\\n\\n境，分析系统数据库）。生产域主要包含各类业务数据产生的源头数据库，分析域\\n\\n主要包含各类数据分析平台和应用。业务系统作为数据生产者，在研发流程中引入\\n\\n数据标准的规范性要求，结合数据库开发工具，从业务系统数据库表结构设计开始\\n\\n进行约束，就如同治理河流上游污染，可让下游污染治理起到事半功倍的作用。全\\n\\n链路数据治理是治“本”。 \\n\\n当然，在全链路数据治理体系下，首先需要建立更强的组织协同能力，数据治理是\\n\\n数据部门主导，而业务研发是科技部门主导，一个追求数据质量，一个追求研发效\\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n40 \\n\\n率，由于目标不同会导致部门间配合上产生相互掣肘，因此需要更高层的组织管理\\n\\n机制进行协同和推动。 \\n\\n•  “享”：服务与共享。 \\n\\n数据资产服务与共享考虑的问题是如何让具体的应用端便捷、高效的实现数据资产\\n\\n在企业内部的充分共享。数据资产的服务与共享工作围绕着数据产品超市、数据服\\n\\n务计价和数据服务运营几个方面展开，其中： \\n\\n数据产品超市。建立数据产品管理体系，数据产品建设完成后，可借鉴“产品超市”\\n\\n思路建设，以清晰和友好的访问形式，开放给数据用户。通过较好的产品设计体验，\\n\\n用户可以快速浏览和选择适合自己需求的数据产品。通过线上获取数据产品访问权\\n\\n限，一键即可开通权限获取数据，提升数据的获取效率和用户体验。 \\n\\n数据资产服务计价。数据资产服务计价，需综合考虑数据资产成本、数据资产业务\\n\\n价值、数据资产市场价值以及现有和预期的服务规模。由于受行业监管和数据安全\\n\\n等因素制约，目前金融行业的对外数据服务发展有限，数据资产服务仍然以内部服\\n\\n务为主。但随着金融科技的输出，更是进一步把服务计价的诉求逐步提上日程。 \\n\\n数据资产服务运营管理。数据资产服务运营的目标是为数据用户提供稳定的、持续\\n\\n的、高质量的数据服务。 \\n\\nd)  用：数据产品与服务打通数据应用“最后一公里” \\n\\n“用”的核心能力在于“价值和体验”，展现高价值数据、降低用数门槛、提升用\\n\\n数体验成为打造数据产品和服务能力的根本目标。“建”好数、“管”好数最终还\\n\\n是为了“用”好数，推动企业内部形成人人都会用数据，人人都是分析师的文化氛\\n\\n围。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n41 \\n\\n数据资产如何为业务提供服务？过往的做法通常是用 BI 报表或数据服务（API）的\\n\\n方式实现，虽然可以满足业务需求，并没有以数据产品的视角，通过产品化设计，\\n\\n沉淀可复用能力和更好的用户体验，通常做法都是按需开发，不仅开发中期长，容\\n\\n易产生数据孤岛，服务体验也不好。为了加快数据服务体系建设，我们提出三个关\\n\\n键方向： \\n\\n一是，数据产品化将成为数据资产从资源态到服务态的重要载体。 \\n\\n通过数据产品化将数据服务能力透传到最终业务用户侧，实现业务价值的完整链路，\\n\\n打通数据应用的“最后一公里”。那么数据产品的价值总体来说在于： \\n\\n首先数据产品从机制上优化了数据服务的供给和需求关系。在构建数据产品化体系\\n\\n之前，数据服务以逐一响应数据服务需求的方式进行生产和供给，大部分是低质量\\n\\n满足的“提数”，单次生产、单次消费，生产速度和质量低效；相似需求，重复响\\n\\n应，如报表建了很多，但访问频次却不高，浪费生产资源。未来在构建数据产品机\\n\\n制后，对相同、相似的需求进行合并和封装，形成“一次研发，重复消费”的高效\\n\\n机制，改善供需关系。 \\n\\n其次数据产品从客观上规范了数据服务标准和服务质量。在构建数据产品体系之前，\\n\\n数据需求的服务响应速度、服务质量高度依赖于提供服务的开发者个人水平。在机\\n\\n制建立后，数据产品本身封装了标准化的服务，保证了服务质量。此外，建立了数\\n\\n据产品的运营机制，保障了服务时效和服务响应率。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n42 \\n\\n最后数据产品从工作方法上加速了数据服务的需求响应效率。之前数据服务当中单\\n\\n纯的“需求响应”工作模式具有很大的差异，数据产品的研发流程以数据产品的机\\n\\n会探查为起点，通过调研、可行性判断，需求管理，原型设计到研发上线，并通过\\n\\n产品的运营不断挖掘新的机会点，形成闭环。 \\n\\n二是，数据产品经理将成为数据人才建设中的“灵魂人物”。 \\n\\n在整个数据产品化过程中贯穿始终，数据产品经理要有敏锐的产品化的“ 机会嗅\\n\\n觉”，能够从复杂的需求中识别出哪些需求是共性的、值得沉淀的，同时又要有产\\n\\n品化思维，从业务需求描述中洞察真正的诉求，并把这种诉求转化为产品的功能设\\n\\n计，以较好的用户体验将服务传递到业务侧，最终形成可上架的数据产品，并建立\\n\\n围绕产品的运营机制，进行持续迭代和优化。前文中已经详细介绍了数字化人才能\\n\\n力建设以及数据产品经理培养路径，这里不再赘述。 \\n\\n借用阿里巴巴集团数据中台一位数据产品经理的自我定位：“产品经理是黑暗中举\\n\\n着火把的人（代表洞见），不仅要照亮前行的路，也要照亮队友脚下的路（代表协\\n\\n同）。产品经理需要通过不断重塑自己，而不断赋予业务新的生命和增长。” \\n\\n三是，一站式数据工作台将加速推动金融机构的数据平民化进程。 \\n\\n数据产品是数据服务业务的一种内容形态，但当一个企业数据部门面对大量的数据\\n\\n服务需求，面对不同受众不同用数据需求时，降低数据使用门槛，让更多的人懂数\\n\\n据，让数据好找，为数据消费者提供便捷、直观、高效的服务，是现代数据建设和\\n\\n管理者开始思考和建设的目标。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n43 \\n\\n通常在金融机构内，数据消费者可分为数据分析者、数据管理者和数据研发者，他\\n\\n们对数据的需求各有侧重。数据分析者希望提供高质量数据，并通过 API 直接访问；\\n\\n数据管理者希望站在全局视角看到企业级数据资产；数据研发者希望提供统一数据\\n\\n研发工具，提升研发效率。 \\n\\n为了满足不同用户的不同需求，应以场景为核心，为数据消费者建立统一的数据门\\n\\n户，整合金融机构已有及新建的数据资产内容，并非是简单页面嵌入，包括数据表、\\n\\n标签、指标、数据 API、数据产品等，通过数据门户打通实现元数据统一管理，并构\\n\\n建数据搜索推荐能力。 \\n\\n通过能力打造，可为数据管理者提供全域数据资产地图，让企业数据底数尽在掌握；\\n\\n可为数据分析师提供统一的数据搜索入口，找数用数更便捷；为数据开发者提供完\\n\\n备的数据开发工具集成，提升数据开发效率，形成开发规范化管理，进一步提高数\\n\\n据质量。除此之外，数据门户还可以成为企业数字化人才培养基地。让数据门户真\\n\\n正成为企业找数据、看数据、用数据的一站式数据工作台。 \\n\\n3)  数字应用 3 个要素 \\n\\n“人、货、场”构建数字化运营体系的核心三要素。 \\n\\n数字基建和数字资产构建了企业核心的数据资产，是数字化转型的基石。那么数字\\n\\n应用则是金融机构真正实现业务数字化、经营管理数字化、组织数字化转型的加速\\n\\n器。 \\n\\n借用阿里巴巴业务矩阵中“消费者运营“的理念，贯穿客户全生命周期、围绕“人\\n\\n-货-场”为核心的数字化运营体系，这个体系的底层逻辑是建立在业务通、数据通、\\n\\n技术通的跨业务单元和技术团队的高效协同上。那么，各大金融机构也早已意识到\\n\\n“以客户为中心”，通过转变业务管理理念，丰富业务场景，提升敏捷+在线化内部\\n\\n管理效率，并与产业生态融合，升级服务体验，不断完善数字化转型的实施策略。 \\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n44 \\n\\na)  “人”：客户经营数字化 \\n\\n大部分金融机构早已建立了以客户为中心的管理体系，建立了围绕客户的标签体系，\\n\\n但从目前运营的结果来看，大多业务经营还是基于业务条线、产品、分支机构等展\\n\\n开，而真正从一个客户视角，对其在一个企业内部整体资产情况、产品偏好、行为\\n\\n习惯等方面进行深度刻画，并形成企业运营管理和分析决策的支撑显然不足。 \\n\\n客户画像作为客户经营重要的数据资产，在精准营销、实时风控等领域具有举足轻\\n\\n重的作用，也是金融机构实现差异化运营和服务的前提。从银行业务视角，客户画\\n\\n像可细分为零售客户、公司客户、机构客户、小微客户、同业客户等，以零售客户\\n\\n画像为例，除了构建基础属性外（如:年龄、性别、职业、地区等）,更重要的还应结\\n\\n合前文介绍的全域埋点数据、数据连接技术和标签体系等，全方位刻画的行为信息、\\n\\n偏好信息等，通过识别不同客户不同阶段的需求，推荐个性化产品。例如：通过客\\n\\n户转账记录分析，针对个人账户从事直播带货的小本经营客户，可挖掘收单、结算、\\n\\n循环授信额度等小微企业的需求。需要说明的是，这些高价值的数据大部分都来源\\n\\n于金融机构内部的交易数据，消除金融机构认为自身数据不足的误区，外部数据仅\\n\\n作为部分补偿和参考。 \\n\\n基于客户行为画像，金融机构才具备开展客户全旅程精细化和数字化运营工作的基\\n\\n础，通过客户行为的真实刻画，让金融机构的经营者可以深度挖掘海量的客户线索，\\n\\n从而进一步围绕客户经营旅程，针对不同客户在不同阶段的活动喜好或需求，推荐\\n\\n差异化活动，包括投资教育、财富管理、养老规划等。 \\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n45 \\n\\n如上图所示，客户经营旅程数字化运营是用数据驱动实现从潜客线索初筛到获客开\\n\\n户再到业务培育最终实现忠诚沉淀的过程，是一条完整链路的客户经营视角，是对\\n\\n客户陪伴服务的过程。在客户初筛及价值定位阶段，将会深度使用之前的客户画像、\\n\\n交易数据、行为数据等各项指标和标签。从最早触达客户开始，就通过数据+AI 技术\\n\\n构建如智能搜索、圈人、推荐的数据智能决策能力。基于数据连接技术识别客户来\\n\\n源，可以对不同来源渠道的潜客进行分级分类，评估潜客质量及其热度，针对客户\\n\\n偏好和需求设计个性化定制产品。根据客户在不同分群中的变化，验证客户经营策\\n\\n略的有效性。通过对客户在全旅程不同节点的反馈变化，定位客户体验不足问题，\\n\\n及时调整策略，提升客户体验。 \\n\\n这些举措的目标始终围绕提升客户经营的数字化，从客户定位-有效开户-培育爬升\\n\\n-忠诚沉淀-以客荐客的长期过程中，通过数据驱动业务流程，优化决策分析，使我\\n\\n们与客户真正形成互助共赢的紧密关系，真正培育出金融机构的长期粘性客户。 \\n\\n除了外部客户外，金融机构还应建立围绕企业高管、内部员工、外部监管、生态客\\n\\n户的经营管理体系，从整个企业服务的对象角度提升数字化服务能力，通过识别\\n\\n“人”，形成围绕“人”的业务目标，如高管关注整个企业经营指标、员工关注敏\\n\\n捷数据服务体验、监管机构关注监管数据质量等，通过不同客户，不同需求建立灵\\n\\n活多样的经营服务体系。 \\n\\nb)  “货”：产品运营精细化 \\n\\n在“货”的建设上，结合客户经营管理的数字化能力，形成不同客群、不同阶段、\\n\\n不同需求的个性化产品运营体系。应用数据分析的算法，通过产品组合提供相应产\\n\\n品建议，并支持客户一键完成购买的交易过程，同时能跟进市场和需求变化情况动\\n\\n态调整，大幅度提升客户体验。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n46 \\n\\n下面以产品定价为例，介绍如何通过数据能力形成的产品精细化运营、客户服务体\\n\\n验、以及跨组织协同能力的全面提升。 \\n\\n过往，金融机构下达经营指标计划是按照业务条线和分支机构进行，并建立考核目\\n\\n标。为了快速完成指标，通常业务拓展时大多选择大客户，但大客户有很强势的议\\n\\n价权，虽然客户营销成功，但金融机构多是赚了规模没赚到效益的结果。 \\n\\n现在，我们通过数据体系的建设，金融机构开始尝试通过数字化经营的思路重新对\\n\\n存量客户进行了分析。首先利用价值评估模型，结合对公客户画像进行客户价值评\\n\\n估，发现企业经营利润多是来源于主营业务为本行的中小企业客户的贡献。经过业\\n\\n务决策，重新制定客户产品定价策略，改变原来按照客户规模进行优惠的策略，而\\n\\n是将价值贡献度高的中小客户设置为优质客群，重新设计产品定价策略，进一步提\\n\\n升客户忠诚度。 \\n\\n同时，由于产品运营的精细化提升，随之客户经营管理也打破了传统以业务条线划\\n\\n分的模式，将原属于公司业务部的中小企业部门和原属于零售部的高端个人业务部\\n\\n门归集成了一个新的客群业务，服务团队也由两个部门的业务人员混编组成，服务\\n\\n团队都可以通过客户经营全旅程的数字化业务流程驱动，实现信息共享、产品服务\\n\\n无缝衔接、客户专享服务体验等方面的提升。 \\n\\n此外，在内部管理中，两个团队仍然沿用原有的管理职能，通过完善效益核算数据\\n\\n使两个团队的绩效合理切分，通过这样跨条线的合作模式组合销售产品，实现客户\\n\\n需求一次性满足，客户满意度和产品交叉销售率都大大提高，激发了两个团队共赢\\n\\n的协同作战，以及敏捷化组织带来的战斗力的提升。 \\n\\nc)  “场”：渠道生态多样化 \\n\\n在“场”的建设上，主要探索多渠道协同经营模式  。通过多样化的渠道建设，实现\\n\\n线上线下一体化，解决金融服务“最后一公里”难题。构建数字渠道核心思想是构\\n\\n建用户承接渠道矩阵，连接用户教育到用户转化，形成和客户的多层次连接，主要\\n\\n包括三层： \\n\\n•  第一层的“场”：通过新媒体流量运营的方式，打造金融机构的品牌心智，比\\n\\n如小红书、抖音、广告投放等渠道传播。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n47 \\n\\n•  第二层“场”：通过类似支付宝、微信公众号等可交互渠道，形成用户和金融\\n\\n机构的交互心智，通过交互加深用户需求理解，产生更强的粘性。 \\n\\n•  第三层“场”：金融自有运营阵地。比如自建的 APP，如手机银行、网上银行\\n\\n等，自建渠道可以是客户享受全链路的金融服务的阵地。 \\n\\n每个场的定位和运营策略都有差异，金融机构可根据客户分层选择适用的渠道开展\\n\\n运营工作，通过不同的“场”完成“人”和“货”的协同。 \\n\\n未来随着数据与 AI 更加深度融合，相信数据将进一步渗透到金融机构数字化经营发\\n\\n展的方方面面。 \\n\\n4)  运营保障 1 套机制 \\n\\n数据体系建设不单纯是技术、平台和产品能力，还需要配套相应的组织运营能力，\\n\\n确保金融机构在数字化能力推进中保持持续发展和迭代。 \\n\\n运营体系的建设需要与金融机构自身的业务战略进行匹配，并作为数据战略的重要\\n\\n组成部分。运营体系的建立包括对应的工作内容与职责、组织保障及人才培养三个\\n\\n层面，围绕战略目标、组织制度、数据产品及服务运营、数据资产运营、及平台工\\n\\n具等维度展开。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n48 \\n\\n运营体系的具体工作，包括数据产品与数据服务运营、数据资产运营、数据平台工\\n\\n具运营。其中：数据产品与数据服务运营需要同业务深度融合，主要工作涉及识别\\n\\n应用场景、提炼业务需求，规划设计数据产品与数据服务、孵化并创造业务价值，\\n\\n数据产品与数据服务推广和运营，持续创造业务价值并打造数据品牌；数据资产运\\n\\n营的具体工作包括全链路标准的制定与完善，建设相应的数据治理体系，实现核心\\n\\n数据资产的沉淀；数据平台工具运营的具体工作包括数据建模、研发、服务一体化\\n\\n平台建设，围绕“采、建、管、用”构建全链路的数据工具持续沉淀与优化。 \\n\\n运营体系的建设往往涉及人员、组织架构和部门职责的调整，其目的就是要集中力\\n\\n量形成组织级的积累，建立可重用的数据能力和培育用数文化。能力与组织架构支\\n\\n撑是数字技术运营体系建设的保障，它对业务、数据、技术等关系人的专业性提出\\n\\n了相应的要求，并需要匹配相应的组织架构来保障，确保建设、运营的顺畅。实践\\n\\n过程中，多家金融机构开始探索敏捷组织模式，设置多专业部门融合跨组织专业团\\n\\n队，如在数字应用的经营管理数字化介绍的场景，通过建立敏捷团队模型形成业务\\n\\n创新的闭环模式，在带来业务价值的同时有效提升组织架构灵活性，让金融企业在\\n\\n数字化时代真正具备差异化核心竞争力。 \\n\\n此外，数字化人才应具备跨部门、跨领域协作能力，掌握跨界知识和数字技术基本\\n\\n知识，具有批判性思维和创新思维，拥有良好的工作与学习自驱力。数字化人才具\\n\\n备对业务或技术观点独立思考能力，并通过分析、比较，进而达到对事物本质更为\\n\\n准确和全面认识的能力，善于利用数据分析结果寻找到最佳解决方案。通过学习自\\n\\n驱力能够自主寻找各种资源、并将新的体验应用于新的创新场景中。因此，运营体\\n\\n系还需要构建学习型组织，持续迭代进化，实现人才数字化能力的可持续发展。 \\n\\n3.  以业务视角的价值链 \\n\\n本章节将以精准获客、产品定价、风险识别、数据管理质量和数据平台增效 5 个场\\n\\n景为例，站在金融机构前台、中台、后台部门，以及数据管理和科技研发部门的不\\n\\n同视角，“看”数据驱动下企业数字化经营的价值体现。 \\n\\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n49 \\n\\n1)  业务前台部门：潜客精准识别 \\n\\n•  痛点 \\n\\n围绕人货场的数字化经营理念，通常银行在对私客户经营模式上相对成熟，而对公\\n\\n客户较为传统，大多还是由客户经理线下拜访，客户关系的维护和拓展新客户大多\\n\\n依赖客户经理自身的能力。通过存量客户再拓新也是一种营销形式，据某个大型银\\n\\n行内部统计，在中小企业领域，每 10 名存量客户就可以成功推荐 1 名新客户开户\\n\\n落地。由此带来的思考是我们的存量客户经常往来的他行客户都有哪些？如果通过\\n\\n数据挖掘的方法，用数据识别存量客户的行为，从而帮助客户经理直接快速地找到\\n\\n他行新客户呢？ \\n\\n•  应用场景 \\n\\n结合客户经营旅程分析方法，带着这样的问题，我们通过对银行自有的交易流水进\\n\\n行了分析，尝试挖掘存量客户的新客线索。通过 150 万存量对公客户的画像分析，\\n\\n找出经营主账户开在该行，且日常经营结算活跃的核心企业客户约 7 万户。然后梳\\n\\n理这些核心客户的交易结算业务，利用知识图谱技术，沿着资金流水去向挖掘其经\\n\\n营上下游的商业伙伴，从而建立客户关系圈，并从中挑选出往来关系密切但尚未在\\n\\n该行开户的客户。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n50 \\n\\n从数据挖掘出的结果来看是惊人的，通过约 7 万户的核心企业客户挖掘出 220 多万\\n\\n上下游往来的他行客户，这个数据甚至远超该行现有存量客户的规模。最终，再结\\n\\n合外部数据，针对 220 多万客户进一步分析，圈选出约 3 万户成为具备发展潜质的\\n\\n优质客群。 \\n\\n•  价值分析 \\n\\n通过对公客户画像和交易行为分析，数智化技术深度挖掘潜客线索，智能化客户价\\n\\n值评估，精准筛选潜客名单，并可直接推动客户经理进行重点营销。通过客户经营\\n\\n数字化能力的提升，改变了原来低效的线下拓客方式，让客户经理营销客户更精准。\\n\\n整个验证过程从取数、建模、打标、分析、圈选等资源花费约 3 个人/月，整体人效\\n\\n比大幅提升。 \\n\\n2)  业务中台部门：产品精确定价 \\n\\n•  痛点 \\n\\n通常银行金融产品的价格管理部门是计财部或资债部，这些部门属于中台部门，对\\n\\n一线的客户并不掌握一手材料。所以在制定价值管理政策时通常是一刀切，或者根\\n\\n据业务一线部门反馈建议修订。前者的弊端是管理与业务脱节，会导致优质客户无\\n\\n法享受优惠价格面临流失，而留下的往往是不在意价格且风险较高的客户。后者把\\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n51 \\n\\n定价权下放到了业务部门，虽然业务更了解客户需求，但弊端是容易造成为了拉动\\n\\n客户，无节制地给予优惠而造成全行效益下滑的问题。所以对于中台部门来说，也\\n\\n同样需要借助数字技术能够清晰了解客户基本信息、行为偏好、价值贡献等方面，\\n\\n建立企业的产品精细化运营体系。 \\n\\n•  应用场景 \\n\\n在前文中已经多次提到，通过数据体系建设，每个企业都构建了客户标签体系，但\\n\\n通过客户价值分析，让中台部门形成的差异化产品定价，并围绕客户经营形成最佳\\n\\n的管理决策能力尚未形成。以一家数字化建设领先的银行为例，他们数据部门采用\\n\\n人工智能技术，通过对业务数据挖掘分析，构建了一套对客评价模型仿真系统，用\\n\\n于总结每个客户的业务特征。具体来说，就是综合运用客户交易行为、产品持有、\\n\\n事件响应等数据构建客户画像，以业务经验为仿真输入，从客户需求、价值、风险\\n\\n三方面深入剖析评价客户，再结合全行管理目标进行客户分群制定经营策略和价格\\n\\n策略。这样设计的价格策略将不再是通过简单规则配置而成，而是既考虑了客户特\\n\\n征，又结合业务目标，同时兼顾了全行经营计划目标的均衡策略。 \\n\\n•  价值分析 \\n\\n通过客群分层价值评估，实现了精细化产品价格管理。针对客户需求、风险、价值\\n\\n贡献制定客户差异化经营策略及定价标准，让管理政策更符合业务需求，做到有序\\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n52 \\n\\n简政放权。经过一段时间运行，资债部价格业务审批量下降了 40%，一线业务部门\\n\\n满意度提升，各客群年度规模、利润、客户增长等全行经营指标均圆满达成。 \\n\\n3)  业务后台部门：风险防患未然 \\n\\n•  痛点 \\n\\n在数据技术爆发的今天，除了业务前台和中台部门，数据还能为后台部门挖掘什么\\n\\n价值？以审计部门为例，金融机构尤其注重对各项业务的合规审计，通常都是从一\\n\\n线业务部门选拔具有丰富经验的人员配置到内控审计部门，究其原因，就是内部人\\n\\n员作案的手段是极其专业且隐蔽的，如果没有任何业务敏感性很难在第一时间发现\\n\\n违规时间的蛛丝马迹。然而内控风险识别，仅凭业务经验往往也是不够的，过去审\\n\\n计人员大多采用调研取证的方式，对于单个已经发生的事件还可行，但针对全机构\\n\\n的业务风险进行及时预警，规避风险的发生就行不通了。 \\n\\n•  应用场景 \\n\\n借助数据体系构建的全域数据资产，我们可以通过挖掘业务数据识别违规事件线索。\\n\\n例如一家银行在日常经营中发现有些客户行为出现异常，沉默多年的账户突然活跃，\\n\\n并出现申请网上小额贷款的业务。 \\n\\n通过数据分析发现，这些账户具有共同特征：都是在县域网点为村民集体办理“农\\n\\n补代发、社保代缴”等业务时开立的账户，账户在开户后很少发生交易行为，而且\\n\\n登记的手机号与户主其他账户登记的手机号都不相同。主要原因是，过去以整村为\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n53 \\n\\n单位的业务办理中，通常是村干部收集全体村民的证件和登记信息，交由客户经理\\n\\n批量办理，这个过程中让别有用心的客户经理滥用客户信息。审计部门通过这个数\\n\\n据线索，挖掘业务记录数据，牵出了客户经理冒用客户资料私自开户并冒名贷款的\\n\\n风险案件。 \\n\\n通过这个案件举一反三，该行分析了全部集体代办业务数据，分析同一客户多部手\\n\\n机、多名客户相同手机号码的各类业务场景，还挖掘了数名内部违规借贷人员。借\\n\\n此，该行优化集体业务办理流程为全部线上化，增加人脸识别、笔迹验真等方式规\\n\\n避了人为操作，既防范风险又提高办理效率。 \\n\\n•  价值分析 \\n\\n在以往纯靠人防控操作风险的时代，这样的小额贷款既不起眼又隐藏多年，通常都\\n\\n是案件发生后由受害人报案才能知道违规事件的发生。而在这个案件中通过客户异\\n\\n常行为指标监测，及时规避风险。审计部门对案例的评价：通过数据能力建设将案\\n\\n件的苗头消灭在萌芽阶段。 \\n\\n4)  数据管理部门：数据高质高效 \\n\\n•  痛点 \\n\\n如何厘清家底，让数据质量更高是数据管理部门面临最大的痛点。数据管理部门是\\n\\n企业数据资产的“大内总管”，经过多年的发展，数据问题依然居高不下，就质量\\n\\n而言，如监管指标二义性问题，在定位和定责方面始终无法从源头解决。前文中介\\n\\n绍过，绝大多数的数据在业务系统中产生，在分析环境中进行深度加工和使用，TP\\n\\n环境作为数据的生产方，AP 系统则是数据消费方和再加工方。 \\n\\n由于业务系统数量较多，来源千差万别，有自研系统，还有外购系统，从数据规范\\n\\n化上很难实现完全的统一，同时即使自研系统由于历史包袱使得改造的难度也非常\\n\\n大，AP 系统也就成为传统数据治理的主战场，管控力度扩散不到 TP 系统。这就如\\n\\n同河流源头的污染源不断在制造污水，中下游的净水器满负荷昼夜不停的运转，辛\\n\\n劳异常却无休无止。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n54 \\n\\n•  应用场景 \\n\\n全链路数据治理的方法就是围绕数据资产的“盘、评、治、享”，开展数据领域的\\n\\n日常工作，促进数据资产的流通和使用。“盘”用于摸清全量家底，“评”用于识\\n\\n别资产价值和成本，“治”用于持续保证数据质量，“享”用于让数据用的更便捷。\\n\\n这些内容是金融机构常年做数据治理追求的目标。 \\n\\n随着数据技术的发展，智能化手段可以实现对数据资产管理能力的增强，比如希望\\n\\n了解企业的全量资产看似容易，实际没有哪家做的非常完美，依然常常出现用数的\\n\\n时候找不到，有数的时候口径对不上，或者每年花费大量的人力投入在治理上。比\\n\\n如，对于全量资产盘点来说，每年的投入人力成本非常可观，但还是不全也不准，\\n\\n我们尝试采用人工智能技术构建自动挂载引擎，通过内容、名称、血缘关系等输入\\n\\n便可自动化实现资产盘点结果与目录的映射，大大提升构建数据资产体系的效率。 \\n\\n再比如，很多金融机构都建设了数据血缘分析，但是指标依然不准，正如痛点中的\\n\\n描述，指标的生成不仅仅来源于 AP 系统，有大量是来源于 TP 系统，指标加工和计\\n\\n算口径往往需要溯源整条链路，如从落标的角度来看，业务系统在新增/修改一张表\\n\\n时，从数据库研发阶段的表结构设计开始，每新增一个栏位都要通过数据标准查找\\n\\n和引用规范性定义，我们利用在线知识图谱技术分别从元数据的物理关系、逻辑关\\n\\n系、加工血缘关系、Schema matching 技术、工单与数据库变更关系、应用与中间\\n\\n件关系、人与数据库的权限关联关系等进行图谱构建，比如当某个数据表要做结构\\n\\n变更时，上下游的依赖及时联动或提醒联动风险，对上下游的数据消费稳定性非常\\n\\n有益，通过管理与工具相结合，实现从数据的生产、存储、传输、加工到计算的全\\n\\n生命周期管理。 \\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n55 \\n\\n•  价值分析 \\n\\n通过全链路数据治理的核心思想把数据开发流程和数据治理流程整合在一起。在数\\n\\n据开发过程中完成数据治理，而不是先开发后治理。全链路数据治理要求数据加工\\n\\n的过程使用数据 MAPPING 来做列级映射，可以从数据产生的最源端的交易型业务\\n\\n系统贯穿到目标端的分析型系统。数据在业务系统和分析系统的存在除了要以表的\\n\\n形式外，还要以最终展示界面的形式提供。 \\n\\n以监管报送指标为例，通过业务系统数据研发规范开始进行管控，利用数据治理工\\n\\n具能力，使得数据治理真正从源头入手进行数据规范性落地，就像是从河流的源头\\n\\n治理污染源，达到“一劳永逸”的效果；其次，利用知识图谱和列算子血缘分析等\\n\\n关键技术，实现精准的数据加工链路定位，指数级提升复杂加工的数据质量溯源（例\\n\\n如：跨多业务条线数据加工的监管报送指标）和问题定位能力。通过实践验证，利\\n\\n用图谱和列算子血缘分析后，可以将原有 SQ 解析成功率从 70%提升到 97%，利用\\n\\n资产目录自动挂载引擎预估将减少 100 人/年的人力费用，一方面大幅提升数据管\\n\\n理人员和分析人员的工作效率，另一方面大幅降低了人力投入成本，真正实现“增\\n\\n效”“降本”的目标。 \\n\\n5)  科技研发部门：算力增效降本 \\n\\n•  痛点 \\n\\n如何让数据回归业务本源是科技部门都要回答的问题。科技部门对于数字技术不应\\n\\n追求大而全，而应将目标聚焦在数字技术的价值释放上，利用成熟技术与创新技术\\n\\n充分融合，明确适用场景，产生可量化、业务有体感的价值。 \\n\\n然而要达到这样的目标并不容易，科技部门在数字技术建设过程中依然面临着很多\\n\\n痛点。例如：如何利用大数据技术、存储技术、网络技术等多项技术改造，优化现\\n\\n有数据平台架构，实现提效降本。科技部门在支撑稳态业务方面已具备了成熟的技\\n\\n术体系与服务流程，但面对日益增加的敏态业务场景，如何利用数字技术进行有效\\n\\n地支撑，提升数据研发效率，缩短从业务需求到产出结果的服务周期。 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          三、金融数据能力建设的核心观点 \\n\\n56 \\n\\n•  应用场景 \\n\\n在数据存储与计算建设方面，在金融行业中已实践了采用存算分离 +湖仓一体相融\\n\\n合的架构进行数据平台的建设。实现了一份数据存储支持多套异构计算引擎，满足\\n\\n不同计算引擎之间的数据共享，避免了数据的多次复制与迁移，有效提升数据加工\\n\\n效率并大大降低存储成本。同时打通不同计算引擎之间的元数据，实现异构引擎之\\n\\n间的数据互访，将计算引擎的优势形成合力。 \\n\\n在支持敏态数据研发方面，与稳态的最大区别在于稳态的开发环境和生产环境是物\\n\\n理隔离的，开发上线有严格的管控流程，适合传统的数据加工场景。而敏态模式对\\n\\n开发和生产资源构建在统一的基础设施上，通过云平台进行开发环境与生产环境的\\n\\n资源逻辑隔离与统一管控；在数据层面，提供数据脱敏与加密的能力，确保数据安\\n\\n全可控；在数据研发层面，实现开发、测试、部署的一键打通，大大提升数据研发\\n\\n效率。我们已看到有多家国有银行、头部保险集团在敏态数据研发方面进行了深入\\n\\n地探索以及落地推广。 \\n\\n•  价值分析 \\n\\n通过存算分析+湖仓一体的技术能力，实现了数据存储与计算的提效降本。在存储方\\n\\n面，大大降低了原始数据多次复制与重复存储，进而有效节省了存储成本，减少数\\n\\n据复制的同时，能够将更多资源投入到满足业务需求的计算场景中，并将计算引擎\\n\\n从元数据层面进行打通，提升了不同引擎之间的资源互访能力，进而提升了计算效\\n\\n率。通过敏态研发能力的建设有效提升数据研发的效能，大大缩短了从提出业务需\\n\\n求到产出数据结果的周期，提升了业务部门对科技的满意度。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          四、金融数据能力建设的成功要素 \\n\\n57 \\n\\n四、金融数据能力建设的成功要素 \\n\\nCapital One 创始人曾说：“我们不是一家银行，我们是一家以数据作为基础战略的\\n\\n公司，只不过我们公司第一个成功的产品碰巧出现在银行业。” \\n\\n数据能力是未来金融企业的第一能力，数据能力越强的企业，边际效应越强、业务\\n\\n扩展越轻松、产品服务越精细化，数据从业务应用的“结果和记录”变为业务应用\\n\\n的“指挥棒”，使数据对业务进一步的支持能力和价值目标得以有效实现。 \\n\\n1.  关键能力和价值方向 \\n\\n金融行业在数据体系在建设上可参考以下价值方向： \\n\\n1)  降低数据建设成本 \\n\\n云原生化的数据技术体系将降低数据存储成本，提升数据计算的效率，通过建设公\\n\\n共层，提升数据的可复用性，避免重复建设，使得数据建设综合成本降低至少 50%\\n\\n以上。 \\n\\n2)  全链路数据集成与治理 \\n\\n面向金融企业全域的数据集成与数据治理，整合全企业的结构化、半结构化及非结\\n\\n构化数据，数据治理从 AP 分析系统前置至 TP 交易系统，打通数据从产生到消费、\\n\\n管理和运营的全链路，确保数据质量。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          四、金融数据能力建设的成功要素 \\n\\n58 \\n\\n3)  一站式数据研发和服务 \\n\\n一站式敏捷数据研发与数据服务、设计即开发、自动化运维等能力大幅提升研发效\\n\\n率，从需求到产生业务可见结果的周期从月->天，自助取数达 80%以上，缩短交付\\n\\n效率。 \\n\\n4)  多样化数据服务体验 \\n\\n多样化的数据服务方式满足不同技术水平的数据分析用户，提升找数效率，降低业\\n\\n务用数的门槛。在更多的业务场景中使用数据作为决策的依据，促进企业内具备数\\n\\n据分析能力的业务人员比率逐年提升。 \\n\\n5)  强化数据产品经理培养 \\n\\n培养一支复合型的数据人才队伍，为数据建设的持续发展提供保障。尤其数据产品\\n\\n经理的培养尤为重要，数据产品经理可以有效拉近业务、技术与数据的距离，使企\\n\\n业重复开发需求逐年减少。 \\n\\n2.  关键路径和成功要素 \\n\\n“任何事情的成功，都不是拿来主义，也不是一蹴而就的”。 \\n\\n各家金融机构在数据体系建设上需要结合自身现状和现有成果进行审视和评估，从\\n\\n业务价值、技术能力、治理水平以及数据体系的运营能力上进行总体评价，规划满\\n\\n足自身发展需求的建设路径，在架构上可采用整体重构、查漏补缺、能力升级等方\\n\\n式，在路径上可分阶段规划中长期目标，采取小步快跑，满足现阶段业务敏捷创新\\n\\n的需求。 \\n\\n下面我们从数据能力建设的几个关键方面提供建设方案参考。 \\n\\n1)  数据能力的价值评估 \\n\\n对于价值评估，金融机构需要借鉴行业标准、同业经验以及第三方权威机构，形成\\n\\n适合自身的数据体系成熟度评估模型，这套评估模型是确保当前阶段的数据体系持\\n\\n续进行问题识别与迭代优化、明确后续建设方向的关键要素。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          四、金融数据能力建设的成功要素 \\n\\n59 \\n\\n数据体系评估模型通常可以包含能力域、能力项以及评估等级。评估模型覆盖数据\\n\\n建设的各个方面，包括数据集成、数据计算、数据资产建设、数据资产管理、数据\\n\\n服务、数据应用以及数据运营几大能力域，在每个能力域中可分解为若干个能力项，\\n\\n每个能力项代表了某个数据能力，并且对应不同的评估等级，不同的评估等级代表\\n\\n了能力的高低。 \\n\\n数据体系评估模型一方面可以对金融机构当前的数据能力现状进行定量与定性相结\\n\\n合的评估，识别出当前建设的优劣势，同时可对未来建设的方向以及建设的重点提\\n\\n供指引与参照，形成可落地的建设路线图。目前行业内已经发布过如 DCMM、EDMM、\\n\\nDataOps 等评估模型，并且有专业的评测机构，也可邀请该领域的优秀企业共创。\\n\\n由于每家金融机构的数据现状和能力差异较大，在通用的评估模型上应结合自身特\\n\\n点形成符合自身发展特点的评估方法，避免生搬硬套。 \\n\\n2)  数据模型的合理选择 \\n\\n前文的数据“建”设中已经详细介绍了关于数据模型的建设方法，那么企业的数据\\n\\n模型选择对于数字应用至关重要，因此这里介绍一下数据中台模型与数仓模型的关\\n\\n键差异，作为企业在选择数据模型建设过程中的指导： \\n\\n•  范式建模 VS 维度建模：传统数仓以范式化建模为主要形式，模型稳定、脚本开\\n\\n发逻辑复杂、周期长；数据中台模型以维度建模为主要形式，开发周期短，上线\\n\\n速度快，但是模型的稳定性不足，会随着业务的变化和模型整体性考虑而频繁\\n\\n调整。 \\n\\n•  数据中台的模型更注重高价值数据建设：在数据中台的模型建设中，更关注高\\n\\n价值数据的萃取和建设，例如标签体系、指标体系等；这些数据包含着更丰富\\n\\n的业务理解和价值沉淀，可以更好的为业务场景服务。 \\n\\n•  数据中台模型建设引入了新技术和新理念：数据中台模型的过程当中引入更多\\n\\n的新技术和理念，例如使用数据连接技术，实现内场景和外场景中客户信息的\\n\\n连接与识别，可以构建更完整的客户视图，从而为风险、营销等应用领域提供\\n\\n更精准的客户画像。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          四、金融数据能力建设的成功要素 \\n\\n60 \\n\\n3)  传统数仓的演进模式 \\n\\n金融机构大数据体系建设已经将近 20 年，技术积累和数据积累非常丰厚，从新的\\n\\n技术发展和数据应用需求来看，建设模式主要分为三种：数仓平迁、数仓升舱、数\\n\\n据中台重构。 \\n\\na)  数仓平迁模式 \\n\\n金融客户基于数据仓库构建了标准的数仓模型，支撑了各种复杂的业务查询、分析、\\n\\n指标报表等业务场景。数据仓库迁移目标主要有两类平台：MPP 数仓和大数据平台，\\n\\n一般是基于物理机存算一体的架构管理。 \\n\\n数据仓库迁移通常包含作业迁移、调度迁移、数据迁移等工作，需要作业加工逻辑\\n\\n一致性校验、数据迁移一致性校验等工作。数仓平迁的基本原则，在数据架构层面\\n\\n尽量保持原数仓的模型和结构不变，任务开发语法和数据类型都保持相对一致。在\\n\\n技术架构层面则更多考虑语法、接口兼容性和应用服务透明性问题。同时，提供标\\n\\n准的迁移模型工具，实现数据表结构和作业的平滑迁移，这样保证迁移简单、时间\\n\\n短，同时对上下游应用影响较小。 \\n\\nb)  数仓升舱模式 \\n\\n新一代云原生数据仓库将具有云原生化、存算分离、弹性扩缩容、实时写入分析、\\n\\n价格成本低、高性能硬件加持、软件自主可控等优势。云原生数仓依托资源池化采\\n\\n用存储分离的架构，不仅可以灵活扩展，还能让计算效率和资源利用效率都最大化，\\n\\n同时也有利于数据共享、打破业务数据壁垒。而在敏捷实时性方面，数据仓库与大\\n\\n数据技术正在快速融合，云数据仓库正在走向湖仓一体，以提供离线实时一体化的\\n\\n数据处理和分析计算能力。 \\n\\n因此，数仓升舱模式，通常是从限制业务发展，形如“经济舱”的传统数仓技术架\\n\\n构，一步升级到“头等舱”，即采用云原生化架构的云原生数仓。就“一步升级”\\n\\n而言，是指在升舱过程中，除了技术架构上的更新换代外，数据架构上也将参考成\\n\\n熟的数据中台建设方法论，优化或重新规划企业公共数据层，进行企业数字资产沉\\n\\n淀。采用模型设计即开发的智能建模工具快速、规范地实现模型落地，形成高价值\\n\\n资产的全域管理及运营能力，将数据以产品或服务的形式发布，让业务系统或用数\\n\\n人员便捷使用数据，借助数仓升舱项目完整构建企业数据能力体系。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          四、金融数据能力建设的成功要素 \\n\\n61 \\n\\n在迁移方案上，考虑到企业已经沉淀大量的数据资产，通常都要回答模型是否需要\\n\\n重构的问题。这里我们可以从模型所提供服务的最终效果反推模型是否需要调整，\\n\\n可以从模型稳定性、服务时效性和数据准确性三个维度评估。大部分企业依然会采\\n\\n用“平迁+重构”的整体策略进行规划，比如现有模型在支持重要决策指标，而且从\\n\\n效率、质量等方面不存在问题的情况下，可参考平迁模式进行数仓迁移；如果需要\\n\\n支持的大部分业务尚未建设数仓或者已经建设的数仓模型不能覆盖应用场景，这样\\n\\n就需要结合数据能力体系规划重新设计并重构数据模型。 \\n\\n同时，在从传统数仓向云原生数仓升级的过程中，还应考虑数据库之间迁移语法兼\\n\\n容性、迁移工具的便捷性问题，比如多种类型数据源之间的数据传输和迁移工具等，\\n\\n它们与数据开发工具共同组成了从迁移可行性评估、数据集成、数据加工、数据建\\n\\n模以及数据服务的一站式传统数据仓库升舱解决方案，确保数仓迁移的成本和风险\\n\\n双降，数据服务的效率和能力双升。 \\n\\nc)  数据中台重构模式 \\n\\n数据中台项目并非一定要推翻重建，原有的大数据平台或数据仓库系统是否需要重\\n\\n构，主要取决于现有平台的能力和能否解决企业在数据领域的核心痛点。建议先行\\n\\n进行评估，然后进行方案设计和决策。判断传统数仓是否需要推倒重建，可以从数\\n\\n据架构、技术架构二个层面进行评估和判断： \\n\\n•  从数据架构上看，传统数仓为了满足存储、计算、分析等各类场景的需要，通常\\n\\n会按 ODS、DWD、DWS、ADM 进行数据模型的分层设计，判断是否需要重建主\\n\\n要看当前模型是否满足规范性、完整性、可复用性和敏捷性的要求，如果不满\\n\\n足则建议借助数据体系建设升级的契机，对数据架构进行重构。 \\n\\n•  从技术架构上看，数据中台带来了技术平台的能力升级，引入了多种不同的技\\n\\n术，包括离线计算、实时计算、交互式分析、数据加速、机器学习、对象存储、\\n\\n云原生等功能，使数据时延更短、分析更快、存储更大，业务更智能，支持业务\\n\\n场景更丰富，成为业务转型的加速器。 \\n\\n总之，考虑到数仓和数据中台之间的兼容性及可迁移性，同时也要考虑业务对数据\\n\\n的强需求，同构平台可以采用平滑迁移的方式，异构平台建议进行重建。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          四、金融数据能力建设的成功要素 \\n\\n62 \\n\\n4)  数据服务的统一协同 \\n\\n金融机构不断探索应用场景对数据的需求，也积累了不同类型的数据，如结构化、\\n\\n半结构化和非结构化数据，并采用不同的技术手段，对多种数据进行存储和加工，\\n\\n通常数据存储方案主要为数据湖和数据仓库。其中，数据湖存储企业原始数据，包\\n\\n括结构化数据（以业务数据为主）和非结构化数据（包括日志文件、图片、音视频\\n\\n等），提供自助分析、用户画像分析、数据挖掘探查、实时数据分析、图分析、高\\n\\n并发查询、历史数据查询等数据服务场景；而数据仓库主要以存储和处理业务系统\\n\\n的结构化数据为主，沉淀企业级数据模型，处理指标、标签，提供 BI 报表、经营分\\n\\n析等数据服务场景。 \\n\\n同时，金融机构为了便于应用场景使用，通常建设统一的数据服务平台提供面向业\\n\\n务的服务能力。统一数据服务从平台功能、业务场景、数据资产、基础设施不同的\\n\\n角度具备不同能力：从平台功能的角度，统一数据服务需要支持 BI、自助分析、报\\n\\n表、数据推送、AI 机器学习、API 服务等功能；从支持业务场景的角度，支持报表、\\n\\n实时大屏、营销、风控、用户画像、客户详单查询等；从数据资产管理的角度，进\\n\\n行数据权限、数据分类、安全审计、数据生命周期管理；从基础设施角度，支持资\\n\\n源弹性伸缩、分析性能强、时效更快、数据高可用等能力。 \\n\\n建设统一数据服务平台，提供统一门户支持数据的资产分类和数据管理，将散落在\\n\\n多种不同平台的数据统一整合，与全域数据打通，解决数据共享、服务性能等问题，\\n\\n并提供稳定的、高效的、安全的全域数据服务能力。 \\n\\n5)  数据责任的全员共识 \\n\\n在金融企业数据建设工作中，业务人员的参与和职责担当是非常关键和重要的。但\\n\\n是现实的情况确实存在部分业务人员认为“数据质量是技术人员的事，不是我的事。”\\n\\n针对这种情况，首先可以从和业务部门最关切的利益入手，引导业务部门站在自己\\n\\n的职责位置上，例如：“小微贷款”口径通常涉及到零售部、三农部和信贷部多方\\n\\n的业务利益，那就从这个指标入手，邀请多方业务部门共同进行竞争来做“小微贷\\n\\n款”的业务管理责任方。通过这一个具体而微小的“争端”，能够让业务人员理解\\n\\n和感知到，数据是为业务服务的，数据工作对业务部门来讲既是义务也是权利，要\\n\\n重视自身在数据工作中的责任。 \\n\\n其次，行政手段也是必要的，前提是要有比较完备的数据治理机制建设。遇到业务\\n\\n部门互相推诿的时候，治理委员会有权指定某业务部门承担具体职责。 \\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                          四、金融数据能力建设的成功要素 \\n\\n63 \\n\\n6)  数据资产的运营共享 \\n\\n数据资产价值评估作为全链路企业数据资产解决方案的关键一环，对于构建数据治\\n\\n理完整链条，指导企业高质量使用数据资产具有重要价值，进而在企业内部形成数\\n\\n据流转与共享，在企业外部为社会提供数据资产的价值，也同时为企业谋取创新型\\n\\n的收益，实现数据的增值。 \\n\\n多维度企业数据资产评估有助于充分释放数据价值。数据资产价值评估目前还没有\\n\\n统一标准和方法，但有主流方法与参考。由于数据资产不具有实物形态，估值时通\\n\\n常类比无形资产进行分析。在行业实践中，无形资产价值的评估方法一般包括成本\\n\\n法、收益法和市场法三种基本方法及其衍生方法。在传统无形资产成本法的基础上，\\n\\n可以综合考虑数据资产的成本与预期使用溢价，加入数据资产价值影响因素对资产\\n\\n价值进行修正，建立一种数据资产价值评估成本法模型。从实践来看，该模型建立\\n\\n一般分为四个阶段，成本价格测算、阶梯价格测算、资产价格评估、资产价格发布。 \\n\\n集团企业通过数据资产化建设，需通过各级数据共享平台向行业各单位或部门提供\\n\\n企业数据资源汇集、共享交互和数据服务，实现跨单位、跨部门、跨地区、跨业务、\\n\\n多层级的数据共享。数据安全是进行数据共享的基本保障，需涵盖共享数据准备、\\n\\n共享数据交互、共享数据使用三个阶段，加强对数据共享全过程的身份鉴别、授权\\n\\n管理等安全保障，确保数据安全。 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c\\n                                                                                                                                                                              尾声 \\n\\n64 \\n\\n尾声 \\n\\n数据领域的技术能力和业务价值探索从未停歇，云计算和大数据的拥抱；大数据和\\n\\nAI 的结合，大模型对 AI 的驱动，让我们对数据的价值更加期待。让我们共同探索数\\n\\n据智能化的下一个十年…… \\n\\n \\n \\n \\n \\n \\n \\n\\x0c\\n阿里云开发者“藏经阁” \\n\\n海量电子手册免费下载 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c', doc_id='f5970f9c-6635-4d85-93e8-f41f30bfb996', embedding=None, doc_hash='1891cd683fcb9ad857aadfbe2bbc8d0ec70789676af7fb03d5aa4114f8d46a9c', extra_info=None)]\n"
     ]
    }
   ],
   "source": [
    "CJKPDFReader = download_loader(\"CJKPDFReader\")\n",
    "loader = CJKPDFReader()\n",
    "# documents = loader.load_data(video_urls=['https://www.bilibili.com/video/BV1KK4y1P7Df/'])\n",
    "documents = loader.load_data(file=Path('/Users/moss/Desktop/aliyun.pdf'))\n",
    "# documents = SimpleDirectoryReader('data').load_data()\n",
    "print(documents)\n",
    "# index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='You exceeded your current quota, please check your plan and billing details.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x12b029a00 state=finished raised RateLimitError>]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/tenacity/__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 382\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/embeddings/openai.py:106\u001B[0m, in \u001B[0;36mget_embedding\u001B[0;34m(text, engine)\u001B[0m\n\u001B[1;32m    105\u001B[0m text \u001B[38;5;241m=\u001B[39m text\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 106\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/openai/api_resources/embedding.py:33\u001B[0m, in \u001B[0;36mEmbedding.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 33\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;66;03m# This is only for the default case.\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    138\u001B[0m (\n\u001B[1;32m    139\u001B[0m     deployment_id,\n\u001B[1;32m    140\u001B[0m     engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m     api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    151\u001B[0m )\n\u001B[0;32m--> 153\u001B[0m response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/openai/api_requestor.py:226\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    216\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[1;32m    217\u001B[0m     method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[1;32m    218\u001B[0m     url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    224\u001B[0m     request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    225\u001B[0m )\n\u001B[0;32m--> 226\u001B[0m resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/openai/api_requestor.py:619\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[0;34m(self, result, stream)\u001B[0m\n\u001B[1;32m    617\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    618\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 619\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    623\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    624\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    625\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    626\u001B[0m     )\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/openai/api_requestor.py:682\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[0;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[0;32m--> 682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[1;32m    683\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[1;32m    684\u001B[0m     )\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[0;31mRateLimitError\u001B[0m: You exceeded your current quota, please check your plan and billing details.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRetryError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m index \u001B[38;5;241m=\u001B[39m GPTSimpleVectorIndex\u001B[38;5;241m.\u001B[39mload_from_disk(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindex.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# index.save_to_disk('index.json')\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# response = index.query(\"What is a summary of this collection of text? Answer in Chinese\", response_mode=\"tree_summarize\")\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# response = index.query(\"用中文详细总结全文并给出重点列表\", response_mode=\"tree_summarize\")\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m什么是数字化运营体系的核心三要素？\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/indices/base.py:417\u001B[0m, in \u001B[0;36mBaseGPTIndex.query\u001B[0;34m(self, query_str, mode, query_transform, use_async, **query_kwargs)\u001B[0m\n\u001B[1;32m    401\u001B[0m query_config \u001B[38;5;241m=\u001B[39m QueryConfig(\n\u001B[1;32m    402\u001B[0m     index_struct_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_struct\u001B[38;5;241m.\u001B[39mget_type(),\n\u001B[1;32m    403\u001B[0m     query_mode\u001B[38;5;241m=\u001B[39mmode_enum,\n\u001B[1;32m    404\u001B[0m     query_kwargs\u001B[38;5;241m=\u001B[39mquery_kwargs,\n\u001B[1;32m    405\u001B[0m )\n\u001B[1;32m    406\u001B[0m query_runner \u001B[38;5;241m=\u001B[39m QueryRunner(\n\u001B[1;32m    407\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_llm_predictor,\n\u001B[1;32m    408\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prompt_helper,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    415\u001B[0m     use_async\u001B[38;5;241m=\u001B[39muse_async,\n\u001B[1;32m    416\u001B[0m )\n\u001B[0;32m--> 417\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mquery_runner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_str\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_index_struct\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/indices/query/query_runner.py:127\u001B[0m, in \u001B[0;36mQueryRunner.query\u001B[0;34m(self, query_str_or_bundle, index_struct)\u001B[0m\n\u001B[1;32m    125\u001B[0m     query_bundle \u001B[38;5;241m=\u001B[39m query_str_or_bundle\n\u001B[1;32m    126\u001B[0m query_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_obj(index_struct)\n\u001B[0;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mquery_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/token_counter/token_counter.py:55\u001B[0m, in \u001B[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001B[0;34m(_self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m start_token_ct \u001B[38;5;241m=\u001B[39m llm_predictor\u001B[38;5;241m.\u001B[39mtotal_tokens_used\n\u001B[1;32m     53\u001B[0m start_embed_token_ct \u001B[38;5;241m=\u001B[39m embed_model\u001B[38;5;241m.\u001B[39mtotal_tokens_used\n\u001B[0;32m---> 55\u001B[0m f_return_val \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_self\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m net_tokens \u001B[38;5;241m=\u001B[39m llm_predictor\u001B[38;5;241m.\u001B[39mtotal_tokens_used \u001B[38;5;241m-\u001B[39m start_token_ct\n\u001B[1;32m     58\u001B[0m llm_predictor\u001B[38;5;241m.\u001B[39mlast_token_usage \u001B[38;5;241m=\u001B[39m net_tokens\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/indices/query/base.py:373\u001B[0m, in \u001B[0;36mBaseGPTIndexQuery.query\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;129m@llm_token_counter\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_bundle: QueryBundle) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RESPONSE_TYPE:\n\u001B[1;32m    372\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 373\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;66;03m# if include_summary is True, then include summary text in answer\u001B[39;00m\n\u001B[1;32m    375\u001B[0m     \u001B[38;5;66;03m# summary text is set through `set_text` on the underlying index.\u001B[39;00m\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;66;03m# TODO: refactor response builder to be in the __init__\u001B[39;00m\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_mode \u001B[38;5;241m!=\u001B[39m ResponseMode\u001B[38;5;241m.\u001B[39mNO_TEXT \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_include_summary:\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/indices/query/base.py:343\u001B[0m, in \u001B[0;36mBaseGPTIndexQuery._query\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001B[39;00m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;66;03m# TODO: remove _query and just use query\u001B[39;00m\n\u001B[0;32m--> 343\u001B[0m tuples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_nodes_and_similarities_for_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# prepare response builder\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_response_builder(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse_builder, query_bundle, tuples)\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/indices/query/base.py:267\u001B[0m, in \u001B[0;36mBaseGPTIndexQuery.get_nodes_and_similarities_for_response\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Get list of tuples of node and similarity for response.\u001B[39;00m\n\u001B[1;32m    261\u001B[0m \n\u001B[1;32m    262\u001B[0m \u001B[38;5;124;03mFirst part of the tuple is the node.\u001B[39;00m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03mSecond part of tuple is the distance from query to the node.\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;124;03mIf not applicable, it's None.\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    266\u001B[0m similarity_tracker \u001B[38;5;241m=\u001B[39m SimilarityTracker()\n\u001B[0;32m--> 267\u001B[0m nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_nodes_for_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msimilarity_tracker\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msimilarity_tracker\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    271\u001B[0m postprocess_info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity_tracker\u001B[39m\u001B[38;5;124m\"\u001B[39m: similarity_tracker}\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node_processor \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_preprocessors:\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/indices/query/vector_store/base.py:45\u001B[0m, in \u001B[0;36mGPTVectorStoreIndexQuery._get_nodes_for_response\u001B[0;34m(self, query_bundle, similarity_tracker)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_nodes_for_response\u001B[39m(\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     42\u001B[0m     query_bundle: QueryBundle,\n\u001B[1;32m     43\u001B[0m     similarity_tracker: Optional[SimilarityTracker] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     44\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Node]:\n\u001B[0;32m---> 45\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_embed_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_agg_embedding_from_queries\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding_strs\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     query_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_vector_store\u001B[38;5;241m.\u001B[39mquery(\n\u001B[1;32m     50\u001B[0m         query_embedding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_similarity_top_k, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_doc_ids\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m query_result\u001B[38;5;241m.\u001B[39mnodes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/embeddings/base.py:79\u001B[0m, in \u001B[0;36mBaseEmbedding.get_agg_embedding_from_queries\u001B[0;34m(self, queries, agg_fn)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_agg_embedding_from_queries\u001B[39m(\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     75\u001B[0m     queries: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m     76\u001B[0m     agg_fn: Optional[Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, List[\u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     77\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m     78\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 79\u001B[0m     query_embeddings \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_query_embedding(query) \u001B[38;5;28;01mfor\u001B[39;00m query \u001B[38;5;129;01min\u001B[39;00m queries]\n\u001B[1;32m     80\u001B[0m     agg_fn \u001B[38;5;241m=\u001B[39m agg_fn \u001B[38;5;129;01mor\u001B[39;00m mean_agg\n\u001B[1;32m     81\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m agg_fn(query_embeddings)\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/embeddings/base.py:79\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_agg_embedding_from_queries\u001B[39m(\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     75\u001B[0m     queries: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m     76\u001B[0m     agg_fn: Optional[Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, List[\u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     77\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m     78\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 79\u001B[0m     query_embeddings \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_query_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m query \u001B[38;5;129;01min\u001B[39;00m queries]\n\u001B[1;32m     80\u001B[0m     agg_fn \u001B[38;5;241m=\u001B[39m agg_fn \u001B[38;5;129;01mor\u001B[39;00m mean_agg\n\u001B[1;32m     81\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m agg_fn(query_embeddings)\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/embeddings/base.py:68\u001B[0m, in \u001B[0;36mBaseEmbedding.get_query_embedding\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_query_embedding\u001B[39m(\u001B[38;5;28mself\u001B[39m, query: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m     67\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get query embedding.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 68\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_query_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m     query_tokens_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenizer(query))\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_total_tokens_used \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query_tokens_count\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/llama_index/embeddings/openai.py:222\u001B[0m, in \u001B[0;36mOpenAIEmbedding._get_query_embedding\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid mode, model combination: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    221\u001B[0m     engine \u001B[38;5;241m=\u001B[39m _QUERY_MODE_MODEL_DICT[key]\n\u001B[0;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/tenacity/__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/tenacity/__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/workspace/ai/context_aug/venv/lib/python3.9/site-packages/tenacity/__init__.py:326\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreraise:\n\u001B[1;32m    325\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m retry_exc\u001B[38;5;241m.\u001B[39mreraise()\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfut\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexception\u001B[39;00m()\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwait:\n\u001B[1;32m    329\u001B[0m     sleep \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwait(retry_state)\n",
      "\u001B[0;31mRetryError\u001B[0m: RetryError[<Future at 0x12b029a00 state=finished raised RateLimitError>]"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "# index.save_to_disk('index.json')\n",
    "\n",
    "# response = index.query(\"What is a summary of this collection of text? Answer in Chinese\", response_mode=\"tree_summarize\")\n",
    "# response = index.query(\"用中文详细总结全文并给出重点列表\", response_mode=\"tree_summarize\")\n",
    "response = index.query(\"什么是数字化运营体系的核心三要素？\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}